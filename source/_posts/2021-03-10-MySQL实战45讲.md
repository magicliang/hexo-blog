---
title: MySQL实战45讲
date: 2021-03-10 14:53:58
tags:
- MySQL
- 数据库
- 存储
---
# 01 | 基础架构：一条SQL查询语句是如何执行的？

连接器管 tcp 连接。全局的权限、配置的修改不会直接在存量 session 里生效。

show processlist 可以显示 Sleep status 的空闲连接。这个命令看起来是管理 process，其实是管理 session。

默认的连接断开时间是 8 小时，是由参数 wait_timeout 控制的。

> 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。

长连接会把本连接临时使用的内存管理在连接对象里（类似 ThreadLocal 之于线程），这些资源在连接断开的时候才释放，解决方案就是：

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。

2. 如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

MySQL 8.0 以后不会再有查询缓存了。


# 02 | 日志系统：一条SQL更新语句是如何执行的？

## 重要的日志模块：redo log

redo log 是 WAL 日志模块。一个事务会把变更写入内存 + Redo log，然后后台线程再把变更更新到磁盘中。

redo log 通常 4 个 1gb 的文件，在写入一个文件满，或者到达 checkpoint 以后，会 flush 数据到磁盘。checkpoint 的意思是，过了这个点，数据的原地变更已经完成（之前的 redo log 不需要再 apply）。

 有了 redo log，就有 crash-safe。

## 重要的日志模块：binlog

两种日志有以下不同：

redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。

redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。

redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

redo log 的两阶段提交：

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。

4. 执行器生成这个操作的binlog，并把binlog写入磁盘。

5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。

prepare 和 commit 的两阶段提交在 MySQL 的本地事务上也有，不独分布式事务上有。但 MySQL 内部自己也有分布式事务。

## 两阶段提交

binlog 本身意味着可以配合备份恢复丢失的记录-这和 redo log 的功能有些像。

由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。

仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？

先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。
但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。
然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。

先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

## 小结

我还跟你介绍了与MySQL日志系统密切相关的“两阶段提交”。**两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案**，即使你不做数据库内核开发，日常开发中也有可能会用到。

近端 prepare -> 远端写 -> 成功后近端 commit

数据库一天一备好，还是一周一备好？一天一备，需要一天的 binlog；一周一备，需要一周的 binlog。**binlog 越多 RTO（恢复目标时间）越长，binlog 越少，要存储备份需要的空间越多。**

# 03 | 事务隔离：为什么你改了我还看不见？

提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转100块钱，而此时你的银行卡只有100块钱。

转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这100块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。

简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。

而且在全部成功和失败以前，不能读到中间状态，要模拟并发编程的原子性。

## 事务隔离的实现

按照[阿里数据库月报][1]的分析，可见性的 readview 的数据来源是：

> 由于在修改聚集索引记录时，总是存储了回滚段指针和事务id，可以通过该指针找到对应的undo
> 记录，通过事务Id来判断记录的可见性。当旧版本记录中的事务id对当前事务而言是不可见时，则继续向前构建，直到找到一个可见的记录或者到达版本链尾部。（关于事务可见性及read
> view，可以参阅我们之前的月报）

当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。**对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到**。

> 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。
> 
> 在MySQL
> 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有20GB，而回滚段有200GB的库。最终只好为了清理回滚段，重建整个库。

![readview-依赖于回滚.jpg](readview-依赖于回滚.jpg)

## 事务的启动方式

> 如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL的事务启动方式有以下几种：
> 
> 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。
> 
> set
> autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit
> 或 rollback 语句，或者断开连接。
> 
> 有些客户端连接框架会默认连接成功后先执行一个set
> autocommit=0的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。
> 
> 因此，我会建议你总是使用set autocommit=1, 通过显式语句的方式来启动事务。
> 
> 但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次
> “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用commit work and chain语法。
> 
> 在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and
> chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

# 5 04 | 深入浅出索引（上）

1. 哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。
2. 有序数组索引只适用于静态存储引擎，在等值查询和范围查询场景中的性能就都非常优秀。
3. 不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案。数据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中，这里我就不再一一展开了。你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。

![InnoDB的索引组织结构.png](InnoDB的索引组织结构.png)

基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。

有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：

1. 只有一个索引；
2. 该索引必须是唯一索引。

你一定看出来了，这就是典型的KV场景。

# 6 05 | 深入浅出索引（下）

![name-age-index示意图.jpg](name-age-index示意图.jpg)

这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。

所以现在你知道了，这段开头的问题里，我们要为高频请求创建(身份证号，姓名）这个联合索引，并用这个索引支持“根据身份证号查询地址”的需求。

那么，如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护(a,b)、(b) 这两个索引。

这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name字段是比age字段大的 ，那我就建议你创建一个（name,age)的联合索引和一个(age)的单字段索引。

而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断（**所以当代的 where 语句，只要能够在索引里过滤，就不会在 server 层过滤**），直接过滤掉不满足条件的记录，减少回表次数。

可以使用`alter table T engine=InnoDB`来重建主键，不必先删除主键再重建主键（理论上可以重建任意主键）。

# 7 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

> 顾名思义，全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read
> lock
> (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。
> 
> 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。
> 
> 官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。
> 
> 你一定在疑惑，有了这个功能，为什么还需要FTWRL呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。
> 
> 所以，single-transaction方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。
> 
> 你也许会问，既然要全库只读，为什么不使用set global
> readonly=true的方式呢？确实readonly方式也可以让全库进入只读状态，但我还是会建议你用FTWRL方式，主要有两个原因：

> 一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。
> 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。
> 业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。
> 
> 但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁。

FTWRL 也可以用来做主从切换。

> MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。
> 
> 表锁的语法是 lock tables … read/write。与FTWRL类似，可以用unlock
> tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock
> tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
> 
> 举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write;
> 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock
> tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。
> 
> 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock
> tables命令来控制并发，毕竟锁住整个表的影响面还是太大。
> 
> 另一类表级的锁是MDL（metadata
> lock)。MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。
> 
> 因此，在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。
> 
> 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
> 
> 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

**所有对表的增删改查操作都需要先申请MDL读锁，就都被锁住，等于这个表现在完全不可读写了。这也是高负载的服务器很难发生备份的原因，备份连接一直都求不到 MDL 锁**

> 基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段？
> 
> 首先我们要解决长事务，事务不提交，就会一直占着MDL锁。在MySQL的information_schema 库的 innodb_trx
> 表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。
> 
> 但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？
> 
> 这时候kill可能未必管用，因为新的请求马上就来了。比较理想的机制是，在alter
> table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。
> 
> MariaDB已经合并了AliSQL的这个功能，所以这两个开源分支目前都支持DDL NOWAIT/WAIT n这个语法。
> 
> ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N
> add column ...

非阻塞等待、计时等待很重要

# 8 07 | 行锁功过：怎么减少行锁对性能的影响？

MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。

## 从两阶段锁说起

> 这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。
> 
> 知道了这个答案，你一定知道了事务A持有的两个记录的行锁，都是在commit的时候才释放的。
> 
> 也就是说，在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。
> 
> 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。我给你举个例子。
> 
> 假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个业务需要涉及到以下操作：
> 
> 从顾客A账户余额中扣除电影票价；
> 
> 给影院B的账户余额增加这张电影票价；
> 
> 记录一条交易日志。
> 
> 也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？
> 
> 试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。
> 
> 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。
> 
> 好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是，这并没有完全解决你的困扰。

MySQL 处理死锁的方式有 2：
1. 看哪个事务先达到 innodb_lock_wait_timeout 定义的超时，自己回滚。
2. 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。

> 每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。

> 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。
> 
> 这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处理。

但这需要处理类似负载均衡的问题，对于单一数据的分片更新是一个复杂问题。

# 9 08 | 事务到底是隔离的还是不隔离的？

> begin/start transaction
> 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start
> transaction with consistent snapshot 这个命令。
> 
> 在MySQL里，有两个“视图”的概念：
> 
> - 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。
> - 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。

## “快照”在MVCC里是怎么工作的？

> 实际上，我们并不需要拷贝出这100G的数据。我们先来看看这个快照是怎么实现的。
> 
> InnoDB里面每个事务有一个唯一的事务ID，叫作transaction
> id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。
> 
> 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction
> id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。
> 
> 也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。
> 
> 如图2所示，就是一个记录被多个事务连续更新后的状态。

![行状态变更图.png](行状态变更图.png)

> 图中虚线框里是同一行数据的4个版本，当前最新版本是V4，k的值是22，它是被transaction id 为25的事务更新的，因此它的row
> trx_id也是25。
> 
> 你可能会问，前面的文章不是说，语句更新会生成undo log（回滚日志）吗？那么，undo log在哪呢？
> 
> 实际上，图2中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而是每次需要的时候根据当前版本和undo
> log计算出来的。比如，需要V2的时候，就是通过V4依次执行U3、U2算出来。
> 
> 明白了多版本和row trx_id的概念后，我们再来想一下，InnoDB是怎么定义那个“100G”的快照的。
> 
> 按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。
> 
> 因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。
> 
> 当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。
> 
> 在实现上，
> InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。
> 
> 数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。
> 
> 这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。
> 
> 而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。
> 
> 这个视图数组把所有的row trx_id 分成了几种不同的情况。

![数据版本可见性规则.png](数据版本可见性规则.png)

> 这样，对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：
> 
> 1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
> 
> 2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
> 
> 3. 如果落在黄色部分，那就包括两种情况  a. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见；  b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。
> 
> 比如，对于图2中的数据来说，如果有一个事务，它的低水位是18，那么当它访问这一行数据时，就会从V4通过U3计算出V3，所以在它看来，这一行的值是11。
> 
> 你看，有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？因为之后的更新，生成的版本一定属于上面的2或者3(a)的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。
> 
> 所以你现在知道了，InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。
> 
> 接下来，我们继续看一下图1中的三个事务，分析下事务A的语句返回的结果，为什么是k=1。
> 
> 这里，我们不妨做如下假设：
> 
> 事务A开始前，系统里面只有一个活跃事务ID是99；
> 
> 事务A、B、C的版本号分别是100、101、102，且当前系统里只有这四个事务；
> 
> 三个事务开始前，(1,1）这一行数据的row trx_id是90。
> 
> 这样，事务A的视图数组就是[99,100], 事务B的视图数组是[99,100,101],
> 事务C的视图数组是[99,100,101,102]。

![事务A查询数据逻辑图.png](事务A查询数据逻辑图.png)

> 从图中可以看到，第一个有效更新是事务C，把数据从(1,1)改成了(1,2)。这时候，这个数据的最新版本的row
> trx_id是102，而90这个版本已经成为了历史版本。
> 
> 第二个有效更新是事务B，把数据从(1,2)改成了(1,3)。这时候，这个数据的最新版本（即row
> trx_id）是101，而102又成为了历史版本。
> 
> 你可能注意到了，在事务A查询的时候，其实事务B还没有提交，但是它生成的(1,3)这个版本已经变成当前版本了。但这个版本对事务A必须是不可见的，否则就变成脏读了。
> 
> 好，现在事务A要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起的。所以，事务A查询语句的读数据流程是这样的：
> 
> - 找到(1,3)的时候，判断出row trx_id=101，比高水位大，处于红色区域，不可见； 接着，找到上一个历史版本，一看row
> - trx_id=102，比高水位大，处于红色区域，不可见； 再往前找，终于找到了（1,1)，它的row
> - trx_id=90，比低水位小，处于绿色区域，可见。
> 这样执行下来，虽然期间这一行数据被修改过，但是事务A不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。
> 
> 这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，用于人肉分析可见性很麻烦。
> 
> 所以，我来给你翻译一下。一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：
> 
> 1. 版本未提交，不可见；
> 
> 2. 版本已提交，但是是在视图创建后提交的，不可见；
> 
> 3. 版本已提交，而且是在视图创建前提交的，可见。
> 
> 现在，我们用这个规则来判断图4中的查询结果，事务A的查询语句的视图数组是在事务A启动的时候生成的，这时候：
> 
> - (1,3)还没提交，属于情况1，不可见；
> - (1,2)虽然提交了，但是是在视图数组创建之后提交的，属于情况2，不可见；
> - (1,1)是在视图数组创建之前提交的，可见。 你看，去掉数字对比后，只用时间先后顺序来判断，分析起来是不是轻松多了。所以，后面我们就都用这个规则来分析。

只有已提交事务会产生（可见的）新版本，否则不会有。

## 更新逻辑

> 细心的同学可能有疑问了：事务B的update语句，如果按照一致性读，好像结果不对哦？
> 
> 你看图5中，事务B的视图数组是先生成的，之后事务C才提交，不是应该看不见(1,2)吗，怎么能算出(1,3)来？

![事务b更新逻辑.png](事务b更新逻辑.png)

> 是的，如果事务B在更新之前查询一次数据，这个查询返回的k的值确实是1。
> 
> 但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务C的更新就丢失了。因此，事务B此时的set
> k=k+1是在（1,2）的基础上进行的操作。
> 
> 所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。
> 
> 因此，在更新的时候，当前读拿到的数据是(1,2)，更新后生成了新版本的数据(1,3)，这个新版本的row trx_id是101。
> 
> 所以，在执行事务B查询语句的时候，一看自己的版本号是101，最新数据的版本号也是101，是自己的更新，可以直接使用，所以查询得到的k的值是3。
> 
> 这里我们提到了一个概念，叫作当前读。其实，除了update语句外，select语句如果加锁，也是当前读。
> 
> 所以，如果把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode 或
> for
> update，也都可以读到版本号是101的数据，返回的k的值是3。下面这两个select语句，就是分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）。

**update 一定会触发当前读，update 成功一定会导致行视图的版本的 row trx_id 成为本事务，不管提不提交都可见。**

> 所以，如果把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode 或
> for
> update，也都可以读到版本号是101的数据，返回的k的值是3。下面这两个select语句，就是分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）。
> 
> mysql> select k from t where id=1 lock in share mode; mysql> select k
> from t where id=1 for update; 再往前一步，假设事务C不是马上提交的，而是变成了下面的事务C’，会怎么样呢？
> 
> 事务C’的不同是，更新后并没有马上提交，在它提交前，事务B的更新语句先发起了。前面说过了，虽然事务C’还没提交，但是(1,2)这个版本也已经生成了，并且是当前的最新版本。那么，事务B的更新语句会怎么处理呢？
> 
> 这时候，我们在上一篇文章中提到的“两阶段锁协议”就要上场了。事务C’没提交，也就是说(1,2)这个版本上的写锁还没释放。而事务B是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务C’释放这个锁，才能继续它的当前读。

![事务B更新逻辑图（配合事务 C）.png](事务B更新逻辑图（配合事务 C）.png)

> 到这里，我们把一致性读、当前读和行锁就串起来了。
> 
> 现在，我们再回到文章开头的问题：事务的可重复读的能力是怎么实现的？
> 
> 可重复读的核心就是一致性读（consistent
> read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。
> 
> 而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：
> 
> 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
> 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

## 小结

InnoDB的行数据有多个版本，每个数据版本有自己的row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据row trx_id和一致性视图确定数据版本的可见性。

- 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
- 对于读提交，查询只承认在语句启动前就已经提交完成的数据；


## 上期问题时间

我在上一篇文章最后，留给你的问题是：怎么删除表的前10000行。比较多的留言都选择了第二种方式，即：在一个连接中循环执行20次 delete from T limit 500。

确实是这样的，第二种方式是相对较好的。

**第一种方式（即：直接执行delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。**

除此之外，还会造成 binlog 过大。大事务是万恶之源。

第三种方式（即：在20个连接中同时执行delete from T limit 500），会人为造成锁冲突。

  [1]: http://mysql.taobao.org/monthly/2015/04/01/
