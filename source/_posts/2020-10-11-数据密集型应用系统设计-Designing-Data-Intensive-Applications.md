---
title: 数据密集型应用系统设计 - Designing Data Intensive Applications
date: 2020-10-11 23:09:02
tags:
- 系统架构
---
数据密集（Data-Intensive）与计算密集（Compute-Intensive）是当今两大负载类型。前者以大数据为代表，后者以深度学习和 HPC 为主要代表。

谨以本书献给那些追逐梦想的人们。

[另一个电子版本。][1]

# 前言

数据密集型应用要处理的瓶颈往往是数据的规模、数据的复杂度和数据产生与变化的速率；与之对应的是计算密集型应用，CPU 往往成为其瓶颈。

本书是关于数据处理系统及其相关技术的（NoSQL、消息队列、缓存、搜索引擎、批处理和流处理框架）。

每一种技术都基于一定的设计理念，而且只适用于特定的场景。

**不要过度优化。**

# 可靠、可扩展与可维护的应用系统

现在的典型系统架构已经很明确了，因为业界已经有成功的案例，对这些组件做了很好的抽象，我们只要做好拿来主义就行了。

## 可靠性（Reliability）

fault tolerance 和 resilience 是系统的容错的体现。

### 硬件故障

对于大型 IDC，即使磁盘的 MTTF 很高，磁盘数量大了以后，每天发生磁盘损坏也是正常的事情。

硬件容错的方案是制造冗余（冗余磁盘、冗余电源）。

软件容错是第二种方式。

### 软件错误

软件错误可以被认为是 bug。检查 bug 的方法就是不断地做契约检查、测试。

### 人为失误

运维错误是系统下线的首要原因。

常见的做法有：

 - 以最小出错的方式来设计系统。
 - 想办法分离最容易出错的地方、容易引发故障的接口。
 - 充分的测试。
 - 当出现人为错误时、提供快速恢复机制。
 - 设置详细而清晰的监控子系统，包括性能指标和错误率。
 - 推行管理流程并加以培训。

## 可扩展性（ Scalability）

如果系统以某种方式增长，我们应对增长的措施有哪些。

### 描述负载

#### Twitter 的例子

Twitter 的高扇出（fan-out）的结构：

2011 年时：
- 用户发送 tweet 可以 达到 12k request/sec
- 用户有 300 k request/sec 的 home timeline 的读请求

用户有不同的扇出结构，决定了他们的潜在写放大的系数。

对于 home timeline 的读，有两种方式可以获取所有内容：

- lazy 方案

这个方案是基础方案，基于 MySQL 的联表查询。

每次每个 follower 读取自己的 home timeline 时，首先 join 自己的 follows 表里的 followee（通过 user_id = follower_id），然后用 followee 去 join user 表（ 通过 followee_id = user_id 这一步其实可以省略），然后用 user 表去 join tweets（通过 user_id = sender_id）。

这种 join 方法可以通过 server side join 来优化，但本质上还是逐步联表。每次做联表查询的时候 join 一次。

如果有必要，这里还可以把 join 的结果缓存起来优化频繁刷新的场景。

这种方法的缺点是，读取大量数据时老老实实地联表查询过多，性能不好。

- eager 方案

这个方案是性能优化方案，基于动态创建的广播队列。

每次每个 followee 发送 tweet 时，会先插入数据到 tweet 表里，然后通过广播的方式把这个 tweet 插入到每个 follower 的一个总的 tweets 列表里。这个列表可以是数据库，也可以是缓存的 list，也可以是 mq 的 topic。因为 mq 的 topic 不适合多对多的生产者和消费者的映射关系，而且动态创建 topic 的成本也很高。缓存的 list（如 redis 的 list）的创建销毁成本很低，很适合这种场景。

这种方案的优点是比方案 1 性能高两个数量级，缺点是如果 fan-out 很大的话，广播的时间会非常长。

因此 Twitter 最后的解决方案是先对大多数 followee 的 tweets 采用方案 2，而对于 fanout 特别多的 followee 的 tweets 使用方案 1，用户最终看到的内容，始终是方案 2 和方案 1 延迟合并的结果。

这个例子可以应用在非常多的 OLAP 场景内：即对于大数据量的数据汇总查询，我们可以优先采取 eager write 或者 broadcast 的方法在写事务的时候插入汇总数据；然后对于 fan-out 特别高的数据，在查询的时候 lazy 查询。选择方案时，需要考虑的因素主要是写成本比较高，还是读成本比较高。如果全量写的不会被全量读，而写成本很高的话，不如用 lazy read ；如果读的场景很高，联表查询出现的比例很高，则适合 eager write。

### 描述性能

批处理系统更看重吞吐量，即每秒处理的记录数；而在线系统更看重响应时间，即客户端从发送请求到接收响应之间的时间差（response time = server side latency + communication overhead）。响应时间不是一个固定的数字，而是一个可度量的数字分布。

我们可以用平均值来说明一些问题，但更多的情况下关注分布，我们使用百分位数（percentile），如 p50、p90、p95、p99。亚马逊使用 p999 来定义起内部服务的响应时间标准。

定义 SLA 有助于我们确定我们的标准，我们要为最慢的响应（tail latencies 长尾效应）优化到什么地步（百分位越高，越难优化）。

排队延迟往往在百分数响应时间中影响很大。因为服务器并行处理的请求优先，正在处理的少数请求可能阻挡后续的请求。这被称为队头阻塞。做负载测试的时候不要**等待队头阻塞（无意中缩短队列长度）**，要尽可能多地发送请求。

实践中，总是会使用滑动窗口来持续监控性能变化。

在实践之中，最慢的响应，决定了用户的 RT。

针对特定级别负载设计的架构不太可能应付超出预设目标 10 倍的实际负载-引入 APM 监控非常重要。

在多台机器上分配负载被称为无共享架构。这种架构易于水平扩展。如果服务负载高度不可预测，则引入自动的弹性扩展是好的，否则手动扩展更能处理意外情况。

超大规模的系统往往针对特定应用而高度定制，很难有一种通用架构。背后取舍的因素很多，如数据读写量、复杂程度、存储量，响应时间要求。

对特定应用而言，通常我们要做出某些假设（在可用性、一致性上做假设，如单元化场景下的弱一致性假设），有所取舍，才能在我们需要获得进展的方面取得结果-我们应该只优化最频繁的操作，或其他亟需我们优化的操作。

**可扩展架构通常是从通用模块逐步构建而来，背后往往有规律可循。**本书将讨论通用模块和常见模式。

## 可维护性

软件的成本在于整个生命周期内持续的维护。而遗留系统总有其过期的原因，很难给出通用的优化建议。

可维护性可以被分为三个方面：

 - 可运维性：运维/运营/SRE 团队易于保持系统平稳。
 - 简单性：新的工程师能够轻松理解系统。
 - 可演化性：能够轻松对系统改进
 
### 可运维性

运维团队可能有很多操作，数据系统设计可以提供如下便利：

 - Observability
 - 文档
 
### 简单性

大泥球应用除了功能以外，还提供很多额外意外的复杂性。这种意外的复杂性是可以消除的-而不必减少功能。

消除复杂性最好的手段之一就是抽象。**抽象可以隐藏大量的细节**，而且**可以对外提供干净、易懂的接口。**

### 可演化性

易于修改的系统，易于演化。我们总是处在不断变化的需求中。

# 数据模型与查询语言

语言的边界就是世界的边界。-《逻辑哲学论》

大多数程序都是通过一层一层叠加数据模型的方式来构建的（如网络协议中不同层使用不同的包）。

不同的数据模型支持的操作不一样，有的操作很好，有的操作很不好-数据结构决定算法，数据结构加算法等于程序。精通一种数据模型需要很大功夫。

## 关系模型与文档模型

### 历史

 Edgar Codd关系型数据库的核心用例最初是商业数据处理，曾经出现过网络模型和层次模型等不同的范式作为竞争对手，但最终关系模型成为最终的赢家。
 
在关系模型里，relation 最终被当作表，行即元组。
 
NoSQL 是关系模型的有力竞争者，最初出现在 Twitter tag 里。它用 schemaless 换取了表达能力的提升，sharding 和 replica 换取了 scalability 的提升。

NoSQL 对 OO 的编程语言的适配性更好。

Linkedin profile 的例子告诉我们，education 和 position 对 user 而言是多对一的关系，可以建模为单独的行，也可以建模为嵌套的文档-因此可以使用 json document 来标表示（这可以转化为 json tree），也可以用关系型数据库的 xml/json 类型来表达。但行业、地区等全局的常量数据，则比较适合用单独的表来存放，使用 id 来引用，而严重不适合冗余存放。

不变的业务 fk、物理 fk 适合冗余，而时间/状态则不适合冗余。冗余可以减少联表查询的复杂度，但也会增加 update 的难度。

IBM 的 IMS 是最初的层次模型，可以很好地处理一对多问题，但不能很好地处理多对多问题-这种困境近似于现在文档数据库遇到的困境。

网络模型的代表是 CODASYL。在 CODASYL 里面每层有多个父节点，因此实现了多对多。在这种模型里，外键是指针，指针不是外键。这种模型按照路径遍历非常麻烦，更新也非常麻烦。

而使用了关系型数据库后，查询优化器会根据索引和表间关系，来生成“访问路径”-也就是执行计划。查询优化器是是一个被持续优化的怪兽。

文档数据库是某种意义上的层次模型-父文档保存了子文档。

文档型数据库的优点：性能更好，模型更像是程序自己的数据结构，**适合一对多模型**。

关系型数据库则强在 join、**多对一和多对多的表达上**。但，只要文档数据库可以通过标识符来引用其他文档，则文档数据库的表达能力并没有因而减弱。

如果原始数据有类似树型/层次/文档的复合结构，则比较适合使用文档数据库；否则应该对数据进行分解（规范化），得到关系型数据库的表。

通常，关系型表的数据结构相关的代码是更复杂的。但，如果需要引用嵌套的数据，则嵌套层次越深，文档型模型越不好用。

通常情况下，流式/批处理框架/消息队列里的 event，也适于使用文档数据库。事实上，除了订单系统里的订单/子订单以外，应该大量数据模型都可以放进文档型数据库里。

如果确实需要 join，则文档数据库的弊端就出现了。反范式化很难维护一致性，而且程序的流程会变复杂，流程变差了。

总而言之，关联性越高和数据库选型的关系是：文档型 -> 关系型 -> 图 数据库。

### 模式灵活性

应该说，文档型数据库有模式灵活性，它支持读时模式（与之相对地，关系型数据库支持的是写时模式）。文档型数据库往往不在写时执行强制模式校验，读时的兼容性必须由读时的应用程序来保证。

关系型数据库因为执行写时校验，所以出现模式变更时，往往需要成本很高的 migrate 操作。

如果外部模式很多，或者模式很易变-最典型的例子，配置型数据，则很适合使用文档型数据库；反之，关系型数据库则要被派上用场。**模式的损害在于，它不易于变动。**

### 数据局部性与性能

文档型数据库还有一个缺点，就是对它更新，需要**原地重写**，写的开销很大，可能引起存储问题。

### 融合的趋势

关系型数据库和文档数据库的融合是大势所趋。当代的 RDBMS 已经可以很好地处理 XML；而一部分的文档型数据库则可以在查询时支持 join（mongo 是在 client 端支持的，这种方案性能不够好，但支持也比不支持强）。 

## 数据查询语言

### 数据库里的查询语言

SQL 其实是种声明式查询语言，而 CODASYL 实际上是命令式。

命令式的查询语言，会把查询过程 HOW 写出来（所以我们经常做的客户端查询，都是命令式的查询），告诉计算机，要按照怎样的特定顺序，执行某些操作（第三步可以被扩展，扩展为 map reduce 的不断串联/并联执行）。

而声明式的查询语言，只会把 what 写出来（LINQ 最为明显），指定查询哪些模式，满足哪些条件，需要做怎样的数据处理/聚合。剩下的查询过程，由查询优化器来推导。

声明式的语言都有一个特性，就是无法/也不需要指定执行的流程的细节，这给了编译器/运行时重排执行流程，甚至并行化执行的机会。-**声明式其实是一种高级抽象，能够实现复杂查询流程的数据库，才能提供很漂亮的声明式查询语言，这体现了架构设计的一种取舍。**

MongoDB 里面的 AST 式的查询语言，本身只是重新发明了一遍 SQL 罢了。

### web 领域的查询语言

即使只在 Web 领域，CSS 代表的声明式语言，也比 JavaScript 代表的命令式查询要优雅很多。

### MapReduce 查询

MapReduce 起源于谷歌，MongoDB 和 couchDB 等文档型数据库也部分支持 MapReduce。

map 是函数式编程里的 collect，而 reduce 则是 fold 或者 inject。

MapReduce 不是声明式查询语言，也不是一个完全命令式的查询 API，而是介于两者之间：查询（及处理）的逻辑用代码片段来表示，这些代码片段会被框架来重用（代码片段的设计思路，也被用于 Stream 这项新兴技术中）。通常我们使用 map 来生成逻辑 KV，然后用 reduce 对相同的 Key 的 value 进行聚合处理。

map reduce 我们使用纯函数，因为没有副作用，所以纯函数的顺序和执行为之是非常自由的。

MapReduce 实际上是一种偏底层的编程模型，需要执行在计算集群上（否则性能并不好）。SQL 是极高层的计算模型，可以通过 MapReduce 来间接实现。当然，这两者之间并不必然有关系。

## 图计算模型

多对多模型是不同数据模型之间的重要区别特征。关系型数据库只适合处理简单的多对多关系，复杂的多对多关系需要使用图模型。

图包括顶点和边，常见的图有：

 - 社交网络
 - Web 图
 - 公路或铁路网
 
图的强大之处在于，它不仅可以存储同构数据，它提供了单个数据存储区中保存完全不同类型对象的一致性方式。

有多种不同但相关的方法可以构建和查询图中的数据，常见的图有属性图（property graph）和三元存储模型（triple-store），相关的查询语言有三种：Cypher、SPARQL 和 Datalog。

图计算模型比关系型数据库或者 CODASYL 更加自由，不需要指定 schema，而任何顶点都可以和其他顶点互联。

### 属性图

在属性图中，每个顶点包括：

 - 唯一的标识符
 - 出边的集合
 - 入边的集合
 - 属性的集合（键-值对）
 
每个边包括：
 - 唯一的标识符
 - 边开始的顶点（尾部的顶点）
 - 边结束的顶点（头部的顶点）
 - 描述两个顶点间关系类型的标签
 - 属性的集合（键-值对）
 
把这两种定义转化为 SQL，可以得到两张表：顶点表和边表。

```SQL
-- 顶点表
CREATE TABLE `vertices` (
  `id` bigint NOT NULL AUTO_INCREMENT COMMENT '物理主键',
  `properties` json DEFAULT NULL COMMENT '顶点属性，适应变化',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 边表
CREATE TABLE `edges` (
  `id` bigint NOT NULL,
  `tail_vertex` bigint NOT NULL COMMENT '尾顶点',
  `head_vertex` bigint NOT NULL COMMENT '头顶点',
  `properties` json DEFAULT NULL COMMENT '属性',
  `label` varchar(45) NOT NULL COMMENT '关系类型的标签',
  PRIMARY KEY (`id`),
  KEY idx_tail_vertex (`tail_vertex`) USING BTREE,
  UNIQUE KEY idx_vertices (`head_vertex`,`tail_vertex`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
```

关于图模型还有一些值得注意的地方：

1. 任何顶点都可以连接到其他顶点。没有模式限制哪种事务可以或者不可以关联。
2. 给定某个顶点，可以高效地得到它的所有入边和出边，从而遍历图，即沿着这些顶点链条一直向前或者向后。
3. 通过对不同的类型的关系使用不同的标签，可以在单个图中存储多种不同类型的信息，同时仍然保持整洁的数据类型。-传统的关系模型难以表达不同国家的不同地区结构和颗粒度。

图是易于演化的，可以动态地往图里添加功能，图可以很容易适应并扩展。

图是一种前程远大，应用场景广泛的技术。

### Cypher 查询语言

Neo4j 是从黑客帝国里诞生的概念，Cypher 是另一个（和密码学里的 Cypher 恰巧同名），这两个名词都是从人名里诞生的。

我们可以先创建库和数据：

```SQL
CREATE 
(NAmerican: location, {name: 'North America', type: 'continent'}),
(USA: location, {name:'United States', type:'country'}),
(Idaho: location, {name:'Idaho', type:'state'}),
(Lucy: person, {name:'Lucy', type:''}),
(Idaho) -[:WITHIN] -> (USA) -[:WITHIN] -> (NAmerica),
(Lucy) - [:BORN_IN] -> (Idaho)
```

相应的查询语句是这样的：

```SQL
MATCH
(person) -[:BORN_IN] -> () - [:WITHIN*o..]-> (us: location, {name:'United States'}),
(person) -[:LIVES_IN] -> () - [:WITHIN*o..]-> (eu: location, {name:'Europe'}),
RETURN person.name
```

这里的式子分两层：第一层在右边，表明这是任意一个处于特定地点的地点，而第二层在左边，表明这是和第一层变量相关联的顶点。person 是一个待求值的变量。

遍历有两种基本思路：
- 从每个 person 开始，沿着出边过滤。
- 从 us 和 eu 这两个顶点开始，沿着入边过滤。

使用声明式的查询语句，可以让查询优化器自由地决定执行策略。

同样地，我们可以用关系型数据库来表达图数据库。但通常， SQL 查询要求我们能够制定 join 的次序和数量；对于图查询，join 操作的数量不是预先确定的。这种不能确定 join 顺序和次数的查询，容易诱发 SQL 的反模式。

WITHIN*o.. 的意思是，沿着 WITHIN 边，遍历 0 次或多次。

在 SQL 1999 中，查询这种可变的遍历路径，可以使用被称为**递归公用表表达式**（即 WITH_RECURSIVE 语法）来表示。

```SQL
-- head vertex 的意思是箭头，这个算法就是递归地从右向左找点（每找到一个点都把点添加进点集合里），而箭头本身带有 withhin 标签。
-- 这个声明式的算法，可以用类似图算法的方法，命令式地实现
WITH RECURSIVE
      -- in_usa 包含所有的美国境内的位置ID
        in_usa(vertex_id) AS (
        SELECT vertex_id FROM vertices WHERE properties ->> 'name' = 'United States'
        UNION
        SELECT edges.tail_vertex FROM edges
          JOIN in_usa ON edges.head_vertex = in_usa.vertex_id
          WHERE edges.label = 'within'
      ),
      -- in_europe 包含所有的欧洲境内的位置ID
        in_europe(vertex_id) AS (
        SELECT vertex_id FROM vertices WHERE properties ->> 'name' = 'Europe'
        UNION
        SELECT edges.tail_vertex FROM edges
          JOIN in_europe ON edges.head_vertex = in_europe.vertex_id
          WHERE edges.label = 'within' ),
      -- born_in_usa 包含了所有类型为Person，且出生在美国的顶点
        born_in_usa(vertex_id) AS (
          SELECT edges.tail_vertex FROM edges
            JOIN in_usa ON edges.head_vertex = in_usa.vertex_id
            WHERE edges.label = 'born_in' ),
      -- lives_in_europe 包含了所有类型为Person，且居住在欧洲的顶点。
        lives_in_europe(vertex_id) AS (
          SELECT edges.tail_vertex FROM edges
            JOIN in_europe ON edges.head_vertex = in_europe.vertex_id
            WHERE edges.label = 'lives_in')
      SELECT vertices.properties ->> 'name'
      FROM vertices
        JOIN born_in_usa ON vertices.vertex_id = born_in_usa.vertex_id
        JOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id;
```

从这个例子可以看出来，SQL 不如 Cypher，SQL 不具备找到一行记录后自递归的方法。

### 三元存储与 SPARQL

三元存储模型几乎等同于属性图模型，他们只是用不同的名词来描述了相同的思想。

在三元组中，所有信息都以非常简单的三部分形式存储（主语，谓语，客体）。

1. 三元组里主体相当于顶点，谓语和宾语相当于 proerpties 中的 key 和 value。
2. 三元组中的主体相当于顶点，谓语是途中的边，客体是头部顶点。

#### 语义网

Datomic 是一个三元组存储（其实是五元组，带有 2 元版本数据）。语义网（Semantic Network）不是三元组，语义网本身没有靠谱的实现，从未实际出现。

#### SPARQL

SPARQL（音“sparkle”）出现得比 Cypher 早，Cypher 的模式匹配是借用 SPARQL 的。

#### Datalog 出现得更早

Datalog 出现得更早，为 Cypher 和 SPARQL 奠定了基础。它是 Datomic 的查询语言。Datalog 是 Prolog 的一些子集。

## 小结

数据的模型发展的脉络，不过是：

树型 -> 文档 -> 关系模型 -> 图

关联越多，越适合使用后面的数据库。关系模型的平衡性最好，可以模拟其他数据模型。从这个顺序来讲，文档模型是关系模型在复杂性上的退化/或者简化。但唯有关系模型是强制使用模式的。

如果需求不断变化，模式可能不断变化，应该尽量选择无模式的数据模型。

每种模型都有自己的查询语言和框架。

这些模型在实现的时候，需要做一些权衡取舍。

# 数据存储和检索

从最基本的层面看，数据库只做两件事情：向它插入数据时，它就保存数据；之后查询时，它应该返回那些数据。

作为应用开发人员，我们大多数情况下不可能从头开始实现一个自己的存储引擎，往往需要从现存的存储引擎中选择一个适合自己的。其他针对事务型工作负载和针对分析型工作负载的存储引擎存在很大的差异。

我们将研究关系型数据库和 NoSQL 数据库，我们将研究两个存储引擎家族，即日志结构的存储引擎和面向页的存储引擎，比如 B-tree（B-tree 是页的组织）。

## 数据库核心：数据结构

数据存储里通常有三种数据结构：日志、页和索引。

### 日志

许多数据库内部都使用日志，日志是一种只支持追加式更新的数据文件。**一个数据库还要处理其他问题：并发控制、回收磁盘空间、错误处理和部分完成写记录等。**但日志始终是一个很有用的机制，被用在很多地方。

### 索引

在日志里面查找结果是不好的，所以引入第二种数据结构-索引。

索引的基本设计思想是，在原始数据上派生额外数据结构，在索引上保留关键数据或者元数据，作为路标，帮助定位想要的数据。

不同的索引支持不同的搜索方式。

索引必然导致写性能下降，因为索引很难使用追加写，但追加写是性能最高的写入方式。

#### 哈希索引

KV 结构随处可见，是构造更多更复杂索引的基础构造块。如，继承/封装 hashmap 是常见的存储方法。

一个特别简单粗暴的例子：Bitcask 的存储格式，使用 csv 来存储 kv 值，使用 hashmap 来存储 key 和文件系统里的 offset 来充当索引。

我们不能只依赖于一个数据文件，这会导致磁盘空间耗尽-**所以我们对大规模存储应该采取分段的形式。**

但使用多段数据，往往意味着数据需要压缩。压缩可以让段变小，因为段被合并后就不会再被修改，所以很适合放进新文件里。这样可以把旧文件段留出来，提供读写支持。

每个段都有自己的内存哈希表。-这里引出了一个范式，一段数据，到底在内存里是怎么被组织的，在硬盘里又是怎么被组织的，可以完全不一样。

这个方法是最简单的方法。但要在实践中行之有效，还要考虑如下问题：

1. 文件格式：CSV 不是最佳的文件格式，二进制才是。
2. 删除记录：如果要删除键和值，在日志里追加一个删除日志是简单的做法（墓碑）。墓碑会让记录在被合并时删除键值的实际内容。
3. 崩溃恢复：从头到尾读取日志是一个方法，快照内存里的 hashmap 是另一个方法-这和 RDB/AOF 的设计思想是很像的，快照可以加速崩溃恢复，快照本身就是 compaction 过的值。
4. 部分写入的记录：文件要加上校验和。
5. 并发控制：**只有一个写线程追加写入（类似 log4j 的设计），多个线程并发读**-单线程后台消费是一种解决并发问题的基本思路。

为什么要使用追加写，而不是原地更新？这应该是几乎所有的存储使用日志配合数据页的解决方案需要回答的问题。

1. 追加写的顺序写性能好。
2. 顺序写的并发控制和崩溃恢复会简单得多-只有一个数据文件很难处理脏数据和正确的数据。
3. 有了文件合并，可以减少数据文件本身的碎片程度-所以数据文件本身还是 要紧凑，不能作为写的中间文件。

内存里的 hash 表有什么缺点？

1. （因为装填因子的存在）内存利用率不高。
2. 区间查询效率不高。

#### SSTables 和 LSM-Tree

SSTables 是排序字符串表，以它优化 Bitcask 的例子的话，会产生如下变化：

1. 每个的日志段里只能存在一个 kv 值，**不按照它们的写入顺序排序，而按照 key 的字典序排序，每个 key 只出现一次（这就像 TreeMap 了）。**
2. 段按照特定的时间段顺序排序，这也就意味着 compaction 多个段的时候，可以按照时间的顺序读取同一个 key，只保留某个 key 的最新值，丢弃其他段里的其他值。
3. 在内存里保存的索引不需要指向所有的 key value 值，只要能够找到特定的段上的区间起止值，就可以找到特定的段上的最新的 kv，**这样我们可以得到一个稀疏的索引**。
4. 我们的值写入永远都是随机写入的，相关的存储引擎是这样工作的：
  1.  **写入先写入内存中的平衡数据结构**（通常是某种平衡树，如红黑树），这被叫作内存表（Mem table）。
  2.  当内存表的大小达到某个阈值以后，将其生成一个 SSTable 写入磁盘中，然后再生成下一个内存表继续供写入（这应该是一个原子切换）。
  3.  如果有读操作，先在内存表里查找，然后按照写顺序查找最新的磁盘段文件、次新的磁盘段文件，以此类推，直到找到目标（或为空）。
  4.  后台周期性地执行段合并与压缩过程，**以合并多个段文件并丢弃那些已被覆盖或者删除的值**。
5. 为了防止数据库崩溃，也要准备 WAL。WAL 使用**纯粹的追加写**，而不是排序写（这样的性能最好）。一段日志对应一段 WAL。

上述算法实质上是 LevelDB（Riak）和 RocksDB �所使用的，可以嵌入其它程序中提供 KV 存储。这两个引擎都收到 Google 的 BigTable 论文的影响（它引入了 SSTable 和 Mem table 两个术语）。

这种索引结构由 LSM-Tree 命名（这一章可能总体上被称作 LSM-Tree 算法）。Lucene 也使用类似的方案存储 term 和相关的 doc。

**总有很多细节，值得深入优化。**如果有个 Key 找不到，则 LSM-Tree 算法的表现可能很慢。这样可以引入布隆过滤器，近似计算集合的内容。

还有其他的策略可以影响甚至决定 SSTables 压缩和合并时的具体顺序和时机。最常见的方式是大小分级和分层压缩。分层压缩是 LevelDB 和 RocksDB 使用的策略，HBase 使用大小分级，Cassandra 同时支持这两种压缩。

在大小压缩中，较新的和较小的 SSTables 被连续合并到较旧和较大的 SSTables 里。

在分层压缩中，键的范围分裂成多个更小的 SSTables，旧数据被移动到单独的“层级”，这样压缩可以逐步并行并节省磁盘空间。

LSM 的基本思想（保存在后台合并的一系列 SSTable）足够简单有效。**即使数据集远远大于可用内存**，它仍然能够正常工作。由于磁盘是顺序写入的，LSM-Tree 可以支持非常高的写入吞吐量。

![LSM-Tree-Algorithm.jpg](LSM-Tree-Algorithm.jpg)

（要实现上面的结构，要特别考虑各种写操作的并发安全性，高性能还在其次）。

#### B-tree

log-structure 日志索引结构正在逐渐受到更多的认可，但目前最广泛使用的索引结构是 B-tree。

从 1970 诞生以来，B-tree **经历了长久的时间的考验**，时至今日，它仍然是几乎所有关系型数据库中的标准索引实现。

B-tree 也按 Key 对键值对排序（证明这个特性极端重要，对于存储而言，区间查找的重要性超乎想象地有用，这也是为什么 hash 类的存储结构使用场景不广泛的原因）。

B-tree 首先把数据库分解成固定大小的块和页（因为操作系统通常使用块作为名称，所以数据库经常使用页），这样更接近底层硬件的设计。页是内部读写的最小单元。

每个页面都有自己的地址。

![B-tree 的例子.png](B-tree 的例子.png)

B-tree 中一个页所包含的子页引用数量称为分支因子（branching factor）。这种设计使得 B-tree 在 O(logn) 的深度上保持平衡。（如果 使用 B+ 树）一个分支因子为 500（通常应该为几百）的 4kb 页只要 4 层就能够存储 256tb 左右的数据。

如何使 B-tree 更可靠？

对 B-tree 的底层写操作基本上是使用新数据来代替磁盘上的旧页，（通常）这不会改变该页的磁盘存储位置。这与 LSM-tree 形成鲜明的对比，**后者只是追加写（实际上在 SSTable 内部还是排序更新文件，但总体上是追加写），新旧替换的操作发生在后台线程的 compaction 里。**

为了做崩溃恢复，B-tree 当然还是需要引入 WAL（而且 WAL 也会进化，从 binlog 进化到 redo log）。

如果要原地更新数据页，还要考虑并发控制问题，所以需要考虑锁存器（一种轻量级锁）；相比之下，日志结构化的方法先显得更简单了，因为它的实际原地更新是在后台发生的。

如何优化 B-tree？

1. 引入 COW，SNAPSHOT Isolation。
2. 保存 key 的缩略信息，节省页空间，这样树具有更高的分支因子，从而减少层数（这就引入了 B+ 树）。
3. 尽量让相邻的页在磁盘上尽量连续。
4. 在叶子上添加额外的指针，这样寻找兄弟不需要找 parent（这也是 B+树的特性）。
5. 分形树引入了一些 log-structure 来减少磁盘寻道。

#### 对比 B-tree 和 LSM-tree

LSM-tree 的优点是什么？

通常认为，LSM-tree 对写入更快，而 B-tree 对读写更快。当然，真正的性能表现只能选取特定的 workload 进行负载测试才能看出来。

磁盘的总带宽是有限的，SSD 的可擦除写的次数是有限的，所以日志结构的写入可能带来的写放大值得关注，至少 compaction 可能降低初始写入的性能。

因为 compaction 的存在，所以 LSM-tree 的的碎片比 B-tree 要少，所以磁盘上的文件通常要更小。这个结论未必对，因为 LSM-tree 里面是存在重复的键值对的，B-tree 没有这种重复的成本。

LSM-tree 的缺点是什么？

compaction 会影响正在进行的读写操作。如果初始写入吞吐量很高，则压缩可能不能真正匹配上它的写入速率。

通常我们不能限制初始写入的速率。

因为多副本的存在，LSM-tree 不具备 B-tree 能够简单地锁住记录而提供事务功能的优点。

#### 其他索引结构

索引包括：

- 主键索引
- 二级索引：值得关注的是 posting-list，或者追加唯一标识使二级索引成为唯一索引的场景。

B-tree 和 log-structure 都可以拿来实现二级索引（甚至主键索引）。

索引中存储的要么是值，要么是堆文件的位置信息。堆文件才是真正存储数据的地方。

有了堆文件，只是更新值而不变更键，可以触发原地变更，否则需要牵扯到更多的文件修改和指针值修改。

但从索引到堆文件的额外跳转意味着**太多的性能损失**，所以聚集索引是很重要的。但创造聚集索引的次数是有限的，**聚集索引和非聚集索引之间的折中是覆盖索引。**覆盖索引只通过索引就可以回答某些简单的查询。

如果要同时查询多个列的信息，需要引入多列索引，但普通的多列索引在处理**复杂的二维搜索**的时候可能出现索引跳跃的问题：

```SQL
    SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079 
                               AND longitude > -0.1162 AND longitude < -0.1004;
```

这种时候可能需要引入专门的空间索引（SPATIAL index），如 R 树（PostgreSQL 支持 R 树查询）。

如果我们使用模糊索引，可以在某个编辑距离内搜索特定的文本。

我们很多数据结构设计起来都是为了**适应磁盘的限制**（注意，这里的磁盘和 SSD 是两样东西），比如 B+tree 的深度和链表结构就是为了适应访问文件块的次数和寻道时间。我们之所以使用磁盘，有至少几个原因：

1. 磁盘可以持久化数据。
2. 磁盘的成本比内存低。
3. 磁盘上的文件更加容易使用外部工具运维-因为已经在进程之外了。

但如果没有磁盘的限制，我们可以得到极大的性能提升。这种内存提升不是因为磁盘的性能比较差，而是因为避免了用写磁盘的格式对内存数据结构进行编码的开销。因为只要内存足够大，虚拟内存可以使使用磁盘的存储引擎充分利用内存。但如果可以自由地使用数据结构，像 Redis 一样的方案可以提供很多样的实现。

有一种类似虚拟内存的页内存管理机制的方案，anti-caching。反缓存把足够冷的记录交换出内存，写入磁盘，再需要时再单独取回。这个方案比页式内存管理好的地方是，颗粒度更低，比操作系统管理内存的方案更有效。

如果将来 NVM 技术得到普及，可能还需要进一步改变存储引擎设计。

## 事务处理与分析处理

事务是在商业数据处理中诞生的，主要指组成一个逻辑单元的一组写操作。广义的事务处理不一定意味着 ACID，只是意味着低延迟的读取和写入。

数据库被广泛用于处理业务交易，也被用于数据分析，这两种模式有显著差异：

- OLTP 基于键处理，每次查询返回少量记录，随机访问，低延迟写入，数据量小。
- OLAP 对大量记录进行汇总（aggregate），通常要搭配 ETL。

这两种模式都需要交互式响应。
SQL 非常灵活，被证明能够同时胜任 OLTP 和 OLAP。但从上世纪 90 年代初期开始了一种趋势，大企业放弃使用 OLTP 系统用于分析目的，儿子单独的数据库上运行分析。这个单独的数据库被称为数据仓库。

### 数据仓库

企业可能有几十种不同的交易处理系统（Transaction Process System）。这些系统每一个都足够复杂，也每一个都非常重要，数据库管理员往往不愿意让业务分析人员在 OLTP 数据库上直接运行临时分析查询，这些查询代价很高，**可能损害并发执行事务的性能**。

数据仓库里包含公司所有 OLTP 系统的只读副本，通过 ETL 流程导入数据。

几乎所有的大型企业都有数据仓库，但是在小型企业中却几乎闻所未闻。所以在小公司里，OLTP 和 OLAP 是隔离的。

单独的数据仓库，可以针对分析访问模式进行优化。值得注意的是，本章前半部分讨论的索引算法适合 OLTP，但不擅长应对分析查询。

### OLTP 数据库和数据仓库之间的差异

有许多图形化的数据分析工具，它们可以生成 SQL 查询、可视化结果并支持分析师探索数据，例如通过诸如向下钻去、切片和切丁等操作。

数仓和 OLTP 系统的相似之处是：他们都有 SQL 接口。这也可以看出 SQL 作为声明式语言，抽象表达能力之强。

目前市面上有商业数据仓库系统通过商业许可销售系统，也有一些开源的 SQL on hadoop 的解决方案，正在逐渐流行。

### 星型与雪花型分析模式

不像事务处理领域广泛使用的不同数据模型，数据仓库在分析型业务上相当公式化地使用星型模型，也成为维度建模（dimensional modeling）。

模式的中心是一张事实表，事实被捕获为单独的事件成为事实表中的每一行。因为事实表在中央而维度表在四周，这个模式被称为星型模式。

每一行里都有很多属性，和引用其他维度表的外键。这些维度表通常代表事件的对象（who）、什么（what）、地点（where）、时间（when）、方法（how）以及原因（why）。

星型模式有个变体，其中维度被进一步被细分为子空间-维度表之间还可以再进一步用外键互相引用。雪花模型比星型模型更加规范化了（normalization）。但大多数分析人员，雪花模型比星型模型更简单。星型 -> 雪花

在典型的数据仓库中，表通常非常宽，事实表通常超过 100 列，有时候有几百列。维度表也可能非常宽。

## 列式存储

如果事实表中有数亿万亿行、PB 大小的数据，则高效地存储和查询这些数据将成为一个具有挑战性的问题。维度表则通常小很多。所以此节中，将主要关注事实表的存储。

虽然事实表通常超过 100 列，但典型的数据仓库查询往往一次只访问其中的 4 或 5 列。

通常，面向行的存储引擎仍然需要（如果不能处理覆盖索引）将所有行都加载进内存中进行加载、解析和过滤。

面向列的存储的想法很简单：不要将一行中的所有值存储在一起，而是将每列中的所有值存储在一起。

面向列的存储布局依赖一组列文件，每个文件以相同顺序保存着数据行-这样实际上会造成存储的稀疏，但制造了存储上的对齐。

### 列压缩

除了仅从磁盘中加载查询所需的列之外，还可以通过压缩数据进一步降低对磁盘吞吐量的要求。列的值序列有很多重复的话，是压缩的好兆头。在数据仓库中特别有效的一种技术是位图编码（bitmap encoding）。

一列如果有 n 种值，则可以有 n 种位图，每个位图上的一位，代表一行在上面是不是有值。这样重复的字面量存储空间被减少到一个比特。而且可以使用游程编码，对 product_sk in (30, 68, 69)这样的查询，可以对位图进行按位或，然后用求等。

这个设计思想可以总结为：拿可枚举值作为位图数量（横向数据有限），事实数量（纵向数据无限）作为比特。

有一个常见的误解：Cassandra 和 HBase 都源于 BigTable，但它们使用列族，将行主键与列族的列存储在一起，并且不使用列压缩。**所以 BigTable 模型仍然面向行**。

### 内存带宽和矢量化处理

面向列的存储有利于 CPU 利用内存带宽，而且快速矢量化处理。

### 列存储中的排序

在列存储中，行的存储顺序不重要，最简单的方法是按照插入顺序保存，这样插入一个新行只是追加到每个列文件。

但单独排序某一列没有意义，如果这样的话无法知道列中的某一项具体属于哪一行。如果我们知道某列中的第 k 项一定属于同一行，基于这种约定我们可以重建一行。

相反，即使数据是按列存储的，它也需要一次排序整行。

只要涉及到排序，我们就要考虑排序键。第一个排序键往往是最重要的，我们通常选择的排序键是日期之类的列，这类列 selectivity 最高。这样我们进行范围查找的时候，解空间可以一下子收敛到很小的范围，加快查找的结果。

排序的另一个优点是，他可以帮助进一步压缩列。有大量重复值的列压缩率最好。第一列的压缩率最好，第二第三列的排序键的情况会更复杂。因为它们的值域里面相邻的重复值，可能因为归属于不同的第一键而被切割得七零八落-这就是局部簇聚性的局限了。

**排序优先级进一步下降的列基本上会呈现接近随机的顺序，因此通常无法压缩。**

### 几种不同的排序

面向列的存储具有多个排序顺序，这有些类似在面向行的存储汇总具有多个二级索引。但面向行的存储，行的数据只在一处，二级索引里保存的是指向行的指针；对于列存储，通常没有任何指向别处数据的指针，只有包含值的列。**拥有多个维度的存储，对保持查询业务的高可用有一定帮助。**

### 列存储的写操作

B-tree 使用原地更新的方式，必然会带来数据页的裂。而列式存储每插入一行就要更新所有列，代价更大，所以选择 LSM-tree 有其必然性。

通常列式存储在内存中使用的数据结构是面向行还是面向列的，无关紧要（这就是为什么可以使用一个 RB-tree 的原因）。

### 聚合：数据立方体和物化视图

未必每个数据仓库都基于列存储（**但数据仓库的事实表如果有几百列，又会倾向于使用列存储**）。

我们通常需要使用聚合方法处理原始数据的很多列，每次都重新处理非常浪费时间。所以这诞生了两类解决方案：
- 物化视图：将之前的查询结果缓存在磁盘上。我们常说的数据血缘表即是这种表。
- 虚拟视图：我们常说的 view，编写查询的快捷方式，隐藏了真实的细节，差异化地管控查询的细节- SQL on hadoop 就是为分布式文件系统设计的虚拟视图。

物化视图的写入成本很高，但查询效果很好。所以数据仓库喜欢用物化视图而RDBMS 喜欢使用虚拟视图。

物化视图的一种常见情况被称为数据立方体。

一个二维的数据立方体是这样的：每个事实只包含两个维度表的外键，每个维度是二维矩阵的一个方向，而二维矩阵格子是事实的完整属性。这样，我们可以沿着任意维度应用聚合操作，得到减少一个维度的总和。

![data-cube.png](data-cube.png)

注意，这个立方体里有一列专门的聚合列，产生了单一维度的聚合格子（这一列不能存储复合值，它必须从属于这一维度），这个聚合列才是加速的关键。

一般来说，事实表的维度不止五个。我们很难想象五维超立方体是什么样子的。我们可以简单想象一下：
- 数据存放在特定的格子里
- 格子上存储了它拥有的所有维度的外键
- 格子的内容就是维度1-维度2-维度3-...-维度 x 限定的事实属性

数据立方体针对某些查询会非常快，因为她已经被预先计算出来了。但它不能解决非特定维度聚合的问题，所以数据仓库还是必须存储原始数据。

## 小结

OLTP 面向用户，OLAP 面向业务分析师。

OLTP 方面，有两个流派的存储引擎：

- 日志结构流派，它追加更新，后台合并数据页。
- 原地更新流派（而不是 B-tree 流派），它原地更新数据页。

# 数据编码与演化

## 数据编码格式

编码模式要处理模式变化，才能兼容新旧系统和新旧数据。

向后兼容：新代码理解老数据。
向前兼容：老代码理解新数据-这一条比较难做到，类似软分叉。


程序通常使用（至少）两种不同的数据表示形式：

1. 在内存中，保存在专门的数据结构中，使用指针优化 cpu 进行高效访问和操作的优化。
2. 在传输和存储时，将其编码为某种**自包含的字节序列**，由于指针对其他进程没有意义，所以这个字节序列看起来与内存中使用的数据结构大不一样。

从 1 到 2 被称为编码，从 2 到 1 被称为解码。

许多编程语言都内置支持将内存中的对象编码为字节序列。但它有种种缺点：
- 不利于异构系统集成
- 容易导致安全问题
- 不利于版本管理（进而处理向前兼容和向后兼容）
- 效率不高

因此我们产生了一些流行的格式，Json、XML 与二进制变体。这些格式处理数字、模式、二进制数据都有一些小小的问题。

明文是前后兼容性最好的格式，也是最不紧凑的格式。所以越是大数据量的场合，越要发明一些新颖的格式来解决容量问题。

## 数据流格式

**进程内通信是共享内存的通信；进程间通信是基于字节序列的数据编码通信。**我们编写程序时进行函数间调用，就是进程内通信；我们编写 API 进行服务间通信，就是进程间通信。

### 基于数据库的数据流

基于数据库的数据流要注意模式演化和兼容性问题。

### 基于服务的数据流：REST 和 PRC

服务器公开的 API 被称为服务，API 通常包括一组标准的协议和数据格式。Web 浏览器、服务器和网站作者都同意这些标准，所以可以使用任何浏览器访问任何网页。

Web 浏览器不是唯一的客户端，移动设备或者桌面计算机上的应用程序也可以向服务器发出网络请求。

此外，服务器本身也可以成为另一项服务的客户端。这种方法可以用于将大型应用程序按照功能区域分解为较小的服务，服务之间通过请求传递数据。这种构建应用程序的方式被称为 SOA 或者微服务架构。

SOA/微服务体系结构的一个关键设计目标是，通过使服务可独立部署和演化，让程序更易于更改和维护。

#### 网络服务

当 HTTP 被用作与服务通信的底层协议时，它被称作 Web 服务。

REST 不是一种协议，而是一种基于 HTTP 原则的设计理念。它强调：

- 简单的数据格式
- 使用 URL 来标识资源
- 并且使用 HTTP 功能进行缓存控制、身份验证和内容类型协商

REST 在微服务架构非常受欢迎。

相比之下，SOAP 是基于 XML 的协议，虽然它最常用于 HTTP，但其设计目的是独立于 HTTP，并避免使用大多数 HTTP 的功能。SOAP 相关的复杂框架通常是 Web Service Framework，被称为 WS-*。因此它的 API 通常使用 WSDL 来生成代码，使用本地类和方法调用来访问远程服务。

SOAP 过于复杂，对于没有 SOAP 供应商支持的编程语言的用户来说，试图与 SOAP 服务集成非常困难。

#### 远程过程调用（RPC）的问题

EJB、RMI、DCOM 和 CORBA，各有其局限性。

**RPC 思想试图使向远程网络服务发出请求看起来与在同一进程中调用编程语言中的函数或方法相同（这种抽象被称作位置透明）。**这种方法有根本的缺陷（我们无法克服这些分布式计算与生俱来的缺陷），网络请求与本地函数调用非常不同：

- 本地函数调用是可预测的，并且成功或失败仅取决于控制的参数。而网络请求不可预测，因此必须有所准备，重试所有的请求。
- 本地函数调用要么返回一个结果，要么抛出一个异常，或者永远不会返回（因为无限循环或进程崩溃）。网络请求返回时可能没有结果。
- 如果重试失败的网络请求，可能会发生请求实际上已完成，只是响应丢失的情况，这又要求我们建立重复数据消除（幂等性）机制。
- 调用本地函数的时间大致相同，而网络请求的耗时则不同。
- 调用本地函数有时候只要传输指针就行（有时候传递数据，或者说，**针对复杂数据量传递指针，简单数据传递数据**），而远程请求则必须全部传递数据。
- 客户端和服务端可能用不同编程语言来实现，所以 RPC 框架必须处理数据类型转换的问题。

#### RPC 的发展方向

RPC 的性能会更好，但 RESTful API 还有其他一些显著的优点：它有利于实验和调试，支持所有的主流编程语言和平台，并且有一个庞大的工具生态系统。

#### RPC 的数据编码和演化

如果 RPC 经常要用于跨组织边界的通信，维护服务的兼容性会变得更加困难。服务的提供者无法控制其客户，也不能强制他们升级。

管理 API 版本的方法有：

- 在 URL 或 HTTP Accept 头中使用版本号
- 使用 API 密钥来标识特定客户端的服务
- 使用单独的管理接口来更新 API 版本的选项

### 基于消息传递的数据流

消息是介于 RPC 和数据库之间的异步消息传递系统。客户端的请求通过低延迟传递到另一个进程中。

与 RPC 相比，消息传递有几个优点：

- 消息代理可以充当缓冲区
- 它可以自动将消息重新发送到崩溃的进程，从而防止消息丢失
- 它避免了发送方需要知道接收方的IP 地址和端口号
- 它支持将一条消息发送给多个接收方
- 它在逻辑上将发送方与接收方分离

与 RPC 的差异在于，消息传递通信通常是单向的。

#### 消息代理

有一类的消息队列，有请求队列，也有回复队列。

#### 分布式 Actor 框架

Actor 模型是被用于单个进程内并发的编程模型。逻辑被封装在 Actor 中，而不是直接处理线程。

每个 Actor 通常代表一个客户端或实体，它可能有某些本地状态（不与其他 Actor 共享），并且它通过发送和接收异步消息与其他 Actor 通信。**不保证消息传送：在某些错误情况下，消息将丢失。**由于每个 actor 一次只处理一条消息，因此不需要担心线程，每个 Actor 都可以由框架独立调度。虽然很相似，但 Goroutine 不同于 Actor，它是 CSP 模型。Actor 和线程也不一样，**Actor 使用自己的时间片，而不是调用方的时间片**。

在分布式 Actor 框架中，这个编程模型被用来跨越多个节点来扩展应用程序。

分布式 Actor 框架实质上是将消息代理和 Actor 编程模型集成到单个框架中。

Actor 可以很好地支持滚动更新。

## 小结

预祝你的应用程序可以快速迭代，顺利部署。

# 分布式数据系统

主要出于以下目的，我们需要在多台机器上分布数据：

- 扩展性
- 容错与高可用性
- 延迟考虑

## 系统扩展能力

我们经常可以拿来垂直扩展的系统，由一个操作系统管理更多的 CPU，内存和磁盘，通过高速内部总线使每个 CPU 都可以访问所有的存储器或磁盘。这种架构被称作共享内存架构（shared memory architecture）。

共享内存架构只能提供有限的容错能力。

另一种方法是共享磁盘架构（shared disk architecture），它拥有多台服务器，每个服务器有个字独立的 cpu，然后将数据存储在可共享访问的磁盘阵列上，服务器和磁盘阵列之间往往通过高速网络连接。

### 无共享架构

采用这种架构（shared nothing）时，运行数据库软件的机器或者虚拟机称为节点。每个节点使用本地的 CPU、内存和磁盘。

本部分将重点放在无共享体系架构上，并不是因为它一定是所有应用的最佳选择，而是因为它需要应用开发者更多的关注和深入理解。例如把数据分布在多节点上，就需要了解在这样一个分布式系统下，背后的权衡设计和隐含限制，数据库并不能魔法般地把所有复杂性都屏蔽起来。

（我们经常做的无状态服务搭配数据库的架构模式是近于共享磁盘架构的，而我们的单元化架构模式是近于无共享架构的。）

### 复制与分区

将数据分布在多个节点上有两种常见方式：复制和分区。

在了解以上概念之后，我们会讨论分布式环境中错综复杂的权衡之道，很可能我们在设计系统时不得不面对这些艰难选择**（没有做过艰难选择，不能算是解决过很难的问题）**。

# 数据复制

数据复制是指通过互联网络在多台机器上保存相同数据的副本。它有几个好处：

- 使数据更接近用户
- 提高冗余
- 读扩展（冗余提供了超量部署，读扩展也降低了延迟，负载和）
- 
如果复制的数据一成不变，那么复制就非常容易：只需将数据复制到每个节点，一次即可搞定。然而所有的技术挑战都在于处理那些持续更改的数据。

目前流行的复制数据变化的方法有：

- 主从复制
- 多主节点复制
- 无节点复制

复制技术存在许多需要折中考虑的地方，比如**采取同步复制还是异步复制，以及如何处理失败的副本**。

数据库复制是个很古老的话题，因为网络的基本约束条件，自始至终没有发生过本质的变化（计算机发展的早期就已经摸清楚了这些基本问题，而且推导出了问题的基本解法）。

实践中大多数开发人员仍然假定数据库只运行在单节点上，分布式数据库成为主流也是最近发生的事情（**长久以来 MySQL 始终是个非分布式数据库，所以才有各种中间件方案**）。

**许多应用开发人员在这方面经验还略显不足，对诸如“最终一致性”等问题存在一些误解。因此，在“复制滞后问题”中，我们会详细讨论最终一致性，包括读自己的写和单调读。**

## 主节点和从节点

每个保存数据库完整数据集的节点称之为副本。当有了多副本，不可避免地会引入一个问题：如何确保所有副本之间的数据是一致的？

1. 指定某一个副本为主副本（或称为主节点）。当客户写数据库时，必须将写请求首先发送给主副本，主副本首先将新数据写入本地存储。
2. 其他副本则全部称为从副本（或称为从节点）。

### 同步复制与异步复制

![同步复制与异步复制.png](同步复制与异步复制.png)

上图的第一个节点显示了同步复制的工作流程，第二个节点显示了异步复制的工作流程。

同步的优点是，如果用户收到看  OK，则所有从节点 OK 了。同步复制的缺点是，如果有从节点阻塞，所有写入都会被阻塞（主从延迟是很常见的情形，所以同步复制产生的写阻塞会很频繁）。

实践中，如果数据库启用了同步复制，通常意味着其中某一个从节点是同步的，而其他节点则是异步模式。万一同步的从节点变得不可用或性能下降，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点（即主节点和一个同步从节点）拥有最新的数据副本。这种配置有时被称为半同步（这项技术可能是 FaceBook 率先提出的）。

主从复制还可以配置为全异步模式，这样配置性能最高，但可能丢失所有尚未复制到从节点的写请求-这是后面要谈到的“复制滞后问题”的体现。

异步模式看起来不靠谱，但还是被广泛使用，特别是从节点数量巨大或者分布于广域地理环境。

复制涉及的问题非常复杂，多副本一致性和共识问题后续再讨论。链式复制（Chain Replication）能够兼容高吞吐和高可用的场景，已经应用在微软的 Azure 存储中。

### 配置新的从节点

新增从节点不能使用直接停机拷贝的方式，除非我们的主节点停写，直至拷贝完成。但停写的时间不能太长，否则高可用不能被保证。所以常见的做法是：

1. 生成一个**一致性快照**，MySQL 使用 innodbbackupex。
2. 将一致性快照应用（Apply）到从节点上。
3. 从节点请求快照点之后发生的数据更改日志。这种日志的快照点位有特别的叫法，PostgreSQL 将其称为“log sequence number”，MySQL 将其称为“binlog coordinates”。
4. 获得日志后，从节点来应用这些快找点之后的所有数据变更。

### 处理节点失效

#### 从节点失效：追赶式恢复

如果发生网络闪断，只要从节点明确知道故障之前处理的最后一笔事务，然后连接到主节点，请求从那笔事务之后中断期间所有的数据变更。

#### 从节点失效：节点失效

**节点切换，极度危险！**

处理主节点故障的情况则比较棘手：选择某个从节点将其提升至主节点；客户端也需要更新，这样之后的写请求会发送给新的主节点，然后其他从节点要接受新的主节点的变更数据，这一过程称之为切换。

故障切换可以手动执行，也可以自动进行。自动切换的步骤通常如下：

1. 确认主节点失效。一般基于超时的心跳机制。
2. 选举新节点。一般基于共识算法选举新节点，需要选出和失效的主节点数据差异最小的从节点。
3. 重新配置系统使新主节点生效。

在上述切换过程中还充满了各种各样的变数：

1. 如果使用了异步的复制，新的主节点未必收到了原主节点的所有数据（任何一种共识算法都解决不了这个问题）。如果这时候新的节点收到了新的冲突的写请求，这时候可能产生脏数据。常见的解决方案（高可用）方法是：**新的主节点丢弃未完成复制的请求，但这可能会未被数据更新持久化的承诺。-另一种强一致性的做法是，在新主和老主确认校验一致之前，禁止新节点的写。这种禁止对可用性伤害很高，如果校验长时间无法完成，集群会瘫痪掉。**
2. 如果在数据库之外有其他系统依赖于数据库的内容并在一起协同使用，丢弃数据的方案就特别危险。**所以强一致性的数据（余额、序列号等交易因子业务）不能使用丢弃后写的数据方案。**
3. 在某些情况下，可能出现两个从节点都认为自己是新主节点，这种情况被称为脑裂。这非常危险，两个主节点都可能接受写请求，并且没有很好地解决冲突的方法（没有办法自动处理，可能需要引入手工处理）。作为一种安全应急方案，有些系统会采取措施来强制关闭其中一个节点。
4. 如何设置合适的超时时间来检测主节点失效呢？主节点失效后，超时时间设置得越长也意味着总体恢复时间就越长。但如果超时时间设置得太短，可能导致很多不必要的切换，遇到网络流量暴增可能会让系统频繁切换而崩溃。-**超时时间太长或者太短都是陷阱，至少应该让超时时间长于一个应用超时时延的 margin。**

### 复制日志的实现

#### 基于语句的复制

这是最基础的方案。

最简单的情况，主节点记录锁所执行的每个写请求（操作语句）并将该操作作为日志发送给从节点。

区块链就是使用这种方案，但这种方案有诸多限制：

- 限制语句必须是确定性的语句。
- 语句本身不能依赖于本地现有数据（自增列），或者会限制所有语句的执行顺序。这实际上仍然会限制事务的并发执行-只有序列化执行能够无措执行。
- 有副作用的语句。

**我们自己做 C/S 类型的数据同步，也可能采取基于语句的复制的方案，如果我们不能解决上面的问题，我们可能会遇到很大的问题。这就看出 Log-Structure 这种设计模式的重要了。**

#### 基于预写日志的复制

所有对数据库写入的字节序列都被记入日志，这种日志就是 WAL。WAL 的缺点就是它描述的数据结果非常底层，某些磁盘块里的某些字节发生了改变，会让复制方案和存储引擎紧密耦合。

#### 基于行的逻辑日志复制

另一种方法是复制和存储引擎采用不同的日志格式，这样复制与存储逻辑剥离。这种复制日志称为逻辑日志。

MySQL 的 binlog 是这样工作的：针对一个事务，产生涉及多行的多条日志记录，并在后面跟着一条记录，指出该事务已提交。

（因为 binlog 是逻辑日志，所以它不与存储引擎绑定，是 Server 层日志。 binlog 特别适合用来跨版本复制是它的一个优点，但不适合特种的存储引擎的问题恢复。）

##### 123

###### 456

#### 基于触发器的控制

这里的触发器不是数据库内置的 trigger，而是应用层程序。基于应用层的程序比较灵活，**也开销更高，而且也更加容易出错**。

## 复制滞后问题（replication lag）

主从复制要求所有的写请求都经过主节点。为了保证主从复制的写吞吐量，所以往往会开启异步复制。只要开启异步复制，都可能产生各种复制滞后，这也就意味着我们必然面对现实中的最终一致性。

### 读自己的写（read-your-writes）

许多应用让用户提交一些数据，接下来查看他们自己提交的内容。

因为（基于主从的）异步复制的存在，所以可能会有一些用户在提交请求后需要（在副本上）查询自己提交的读。这就需要 read-after-write-consistency。

那么如何保证这种一致性呢？

- 如果用户可能访问会被修改的内容，从主节点读（**读主库是一种写后读，如果使用单元化等方案发生主从切换，不能保证写后读一致性，则强一致性无从保证**）；否则，在从节点读取。这需要在业务规则上区分，什么样的数据是属于用户自己的某次修改的，什么样的数据是属于别人的修改的。
- 如果大部分的内容可能被大部分用户修改，那么尚书方法将不太有效。如果大部分的数据都需要经由主节点，这就丧失了读操作的扩展性。这时候可以尝试（一个弱时间假设），在更新后一分钟的时间内，读主节点；其他时间读从节点。这就要求：我们的时间假设掐得非常准，也要我们监控那些特别慢的节点。
- 客户端可以记住最近更新时的时间戳，并附在读请求中，据此信息，应用可以确定此时读出的数据是不是足够新。如果不够新，要么交给其他副本读，要么等待直至读到相应时间戳的数据为止。这个时间戳可以是逻辑时间戳（日志序列号），也可以是实际系统时钟。通常采取这种方案的系统要具有“写事件驱动读带有时间戳”的特点。
- 如果副本分布在多数据中心，情况复杂些，必须先把请求路由到主节点所在的数据中心（该数据中心可能离用户很远）。

### 单调读（monotonic-read）

单调读的定义是，一个用户每次读不应该看到回滚现象。

单调读是一种比强一致性弱，但比最终一致性强的保证。至此四种一致性之间的关系为：强一致性、单调读、最终一致性、（一般）弱一致性。

保证单调读的方法是让用户每次在一个分片上读数据-换言之，使用随机负载均衡的方案配合主从复制的读扩展，没有办法保证单调读。

### 前缀一致读（consistent prefix reads）

前缀一致读是一种保证，对于一系列按照某个顺序发生的写操作，那么读取这些内容时也会按照当时写入的顺序。

这是分片数据库在多分片写时特有的一个问题。一般情况下，即使以相同的顺序写入数据库，读取也无法保证保证总是看到一致的序列-这和 kafka 多分区无法保证全局有序是一样的。实现前缀一致读的最简单的方法是破坏分区写这个先决条件，保证有因果关系的写入都交给一个分区来完成。

**所以全局有序性 = producer's consistent prefix read + consumer's monotonic read
如果使用单元化/Set 化方案，只要发生主从切换，则无法保证单调读。**

### 复制滞后的解决方案

使用最终一致性系统时，最好先思考这样的问题：（系统最大的风险是，）如果复制延迟增加到几分钟甚至几小时，那么应用层的行为会是什么样子？

如果需要强一致性设计，需要考虑写后读的一致性（写后读也不是很简单的，这里的写后读专指能读到写的写后读）；如果系统设计时假定是同步复制，但最终它事实上成为了异步复制，就可能会导致灾难性后果。

如果需要做特别的设计，需要在应用层上做，应用层也可以提供更灵活、强力的保障措施；而代价则是，应用层中处理这些代码非常复杂、且容易出错。

如果应用程序开发人员不必担心这么多底层的复制问题，而是假定数据库在做“正确的事情”，情况就变得简单。而这也是事务存在的原因，事务是数据库提供更强保证的一种方式。

在单节点上支持事务已经非常成熟，然而在转向分布式数据库（即支持复制和分区）的过程中，有许多系统却选择放弃支持事务，并生成事务在性能与可用性方便代价过高，然后断言在可扩展的分布式系统中最终的一致性是无法避免的终极选择。关于这样的表述，首先它有一定的道理，但情况远不是它所声称的那么简单。-**我们应该跳出一个又一个有过取舍的实现，形成一个一般的、成熟的观点。**

## 多主节点复制

主从复制的方法比较常见。

主从复制存在一个明显的缺点：系统只有一个主节点，而所有写入都必须经由主节点。只要发生单点故障，主从复制就 hi 影响所有的写入操作。

对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似：处理写的每个主节点都必须将该数据更改转发到所有其他节点，这就是多主节点复制。

### 适用场景

在一个数据中心内部使用多主节点基本没有太大意义。

#### 多数据中心

