---
title: 数据密集型应用系统设计 - Designing Data Intensive Applications
date: 2020-10-11 23:09:02
tags:
- 系统架构
---
数据密集（Data-Intensive）与计算密集（Compute-Intensive）是当今两大负载类型。前者以大数据为代表，后者以深度学习和 HPC 为主要代表。

谨以本书献给那些追逐梦想的人们。

# 前言

数据密集型应用要处理的瓶颈往往是数据的规模、数据的复杂度和数据产生与变化的速率；与之对应的是计算密集型应用，CPU 往往成为其瓶颈。

本书是关于数据处理系统及其相关技术的（NoSQL、消息队列、缓存、搜索引擎、批处理和流处理框架）。

每一种技术都基于一定的设计理念，而且只适用于特定的场景。

**不要过度优化。**

# 可靠、可扩展与可维护的应用系统

现在的典型系统架构已经很明确了，因为业界已经有成功的案例，对这些组件做了很好的抽象，我们只要做好拿来主义就行了。

## 可靠性（Reliability）

fault tolerance 和 resilience 是系统的容错的体现。

### 硬件故障

对于大型 IDC，即使磁盘的 MTTF 很高，磁盘数量大了以后，每天发生磁盘损坏也是正常的事情。

硬件容错的方案是制造冗余（冗余磁盘、冗余电源）。

软件容错是第二种方式。

### 软件错误

软件错误可以被认为是 bug。检查 bug 的方法就是不断地做契约检查、测试。

### 人为失误

运维错误是系统下线的首要原因。

常见的做法有：

 - 以最小出错的方式来设计系统。
 - 想办法分离最容易出错的地方、容易引发故障的接口。
 - 充分的测试。
 - 当出现人为错误时、提供快速恢复机制。
 - 设置详细而清晰的监控子系统，包括性能指标和错误率。
 - 推行管理流程并加以培训。

## 可扩展性（ Scalability）

如果系统以某种方式增长，我们应对增长的措施有哪些。

### 描述负载

#### Twitter 的例子

Twitter 的高扇出（fan-out）的结构：

2011 年时：
- 用户发送 tweet 可以 达到 12k request/sec
- 用户有 300 k request/sec 的 home timeline 的读请求

用户有不同的扇出结构，决定了他们的潜在写放大的系数。

对于 home timeline 的读，有两种方式可以获取所有内容：

- lazy 方案

这个方案是基础方案，基于 MySQL 的联表查询。

每次每个 follower 读取自己的 home timeline 时，首先 join 自己的 follows 表里的 followee（通过 user_id = follower_id），然后用 followee 去 join user 表（ 通过 followee_id = user_id 这一步其实可以省略），然后用 user 表去 join tweets（通过 user_id = sender_id）。

这种 join 方法可以通过 server side join 来优化，但本质上还是逐步联表。每次做联表查询的时候 join 一次。

如果有必要，这里还可以把 join 的结果缓存起来优化频繁刷新的场景。

这种方法的缺点是，读取大量数据时老老实实地联表查询过多，性能不好。

- eager 方案

这个方案是性能优化方案，基于动态创建的广播队列。

每次每个 followee 发送 tweet 时，会先插入数据到 tweet 表里，然后通过广播的方式把这个 tweet 插入到每个 follower 的一个总的 tweets 列表里。这个列表可以是数据库，也可以是缓存的 list，也可以是 mq 的 topic。因为 mq 的 topic 不适合多对多的生产者和消费者的映射关系，而且动态创建 topic 的成本也很高。缓存的 list（如 redis 的 list）的创建销毁成本很低，很适合这种场景。

这种方案的优点是比方案 1 性能高两个数量级，缺点是如果 fan-out 很大的话，广播的时间会非常长。

因此 Twitter 最后的解决方案是先对大多数 followee 的 tweets 采用方案 2，而对于 fanout 特别多的 followee 的 tweets 使用方案 1，用户最终看到的内容，始终是方案 2 和方案 1 延迟合并的结果。

这个例子可以应用在非常多的 OLAP 场景内：即对于大数据量的数据汇总查询，我们可以优先采取 eager write 或者 broadcast 的方法在写事务的时候插入汇总数据；然后对于 fan-out 特别高的数据，在查询的时候 lazy 查询。选择方案时，需要考虑的因素主要是写成本比较高，还是读成本比较高。如果全量写的不会被全量读，而写成本很高的话，不如用 lazy read ；如果读的场景很高，联表查询出现的比例很高，则适合 eager write。

### 描述性能

批处理系统更看重吞吐量，即每秒处理的记录数；而在线系统更看重响应时间，即客户端从发送请求到接收响应之间的时间差（response time = server side latency + communication overhead）。响应时间不是一个固定的数字，而是一个可度量的数字分布。

我们可以用平均值来说明一些问题，但更多的情况下关注分布，我们使用百分位数（percentile），如 p50、p90、p95、p99。亚马逊使用 p999 来定义起内部服务的响应时间标准。

定义 SLA 有助于我们确定我们的标准，我们要为最慢的响应（tail latencies 长尾效应）优化到什么地步（百分位越高，越难优化）。

排队延迟往往在百分数响应时间中影响很大。因为服务器并行处理的请求优先，正在处理的少数请求可能阻挡后续的请求。这被称为队头阻塞。做负载测试的时候不要**等待队头阻塞（无意中缩短队列长度）**，要尽可能多地发送请求。

实践中，总是会使用滑动窗口来持续监控性能变化。

在实践之中，最慢的响应，决定了用户的 RT。

针对特定级别负载设计的架构不太可能应付超出预设目标 10 倍的实际负载-引入 APM 监控非常重要。

在多台机器上分配负载被称为无共享架构。这种架构易于水平扩展。如果服务负载高度不可预测，则引入自动的弹性扩展是好的，否则手动扩展更能处理意外情况。

超大规模的系统往往针对特定应用而高度定制，很难有一种通用架构。背后取舍的因素很多，如数据读写量、复杂程度、存储量，响应时间要求。

对特定应用而言，通常我们要做出某些假设（在可用性、一致性上做假设，如单元化场景下的弱一致性假设），有所取舍，才能在我们需要获得进展的方面取得结果-我们应该只优化最频繁的操作，或其他亟需我们优化的操作。

**可扩展架构通常是从通用模块逐步构建而来，背后往往有规律可循。**本书将讨论通用模块和常见模式。

## 可维护性

软件的成本在于整个生命周期内持续的维护。而遗留系统总有其过期的原因，很难给出通用的优化建议。

可维护性可以被分为三个方面：

 - 可运维性：运维/运营/SRE 团队易于保持系统平稳。
 - 简单性：新的工程师能够轻松理解系统。
 - 可演化性：能够轻松对系统改进
 
### 可运维性

运维团队可能有很多操作，数据系统设计可以提供如下便利：

 - Observability
 - 文档
 
### 简单性

大泥球应用除了功能以外，还提供很多额外意外的复杂性。这种意外的复杂性是可以消除的-而不必减少功能。

消除复杂性最好的手段之一就是抽象。**抽象可以隐藏大量的细节**，而且**可以对外提供干净、易懂的接口。**

### 可演化性

易于修改的系统，易于演化。我们总是处在不断变化的需求中。

# 数据模型与查询语言

语言的边界就是世界的边界。-《逻辑哲学论》

大多数程序都是通过一层一层叠加数据模型的方式来构建的（如网络协议中不同层使用不同的包）。

不同的数据模型支持的操作不一样，有的操作很好，有的操作很不好-数据结构决定算法，数据结构加算法等于程序。精通一种数据模型需要很大功夫。

## 关系模型与文档模型

### 历史

 Edgar Codd关系型数据库的核心用例最初是商业数据处理，曾经出现过网络模型和层次模型等不同的范式作为竞争对手，但最终关系模型成为最终的赢家。
 
在关系模型里，relation 最终被当作表，行即元组。
 
NoSQL 是关系模型的有力竞争者，最初出现在 Twitter tag 里。它用 schemaless 换取了表达能力的提升，sharding 和 replica 换取了 scalability 的提升。

NoSQL 对 OO 的编程语言的适配性更好。

Linkedin profile 的例子告诉我们，education 和 position 对 user 而言是多对一的关系，可以建模为单独的行，也可以建模为嵌套的文档-因此可以使用 json document 来标表示（这可以转化为 json tree），也可以用关系型数据库的 xml/json 类型来表达。但行业、地区等全局的常量数据，则比较适合用单独的表来存放，使用 id 来引用，而严重不适合冗余存放。

不变的业务 fk、物理 fk 适合冗余，而时间/状态则不适合冗余。冗余可以减少联表查询的复杂度，但也会增加 update 的难度。

IBM 的 IMS 是最初的层次模型，可以很好地处理一对多问题，但不能很好地处理多对多问题-这种困境近似于现在文档数据库遇到的困境。

网络模型的代表是 CODASYL。在 CODASYL 里面每层有多个父节点，因此实现了多对多。在这种模型里，外键是指针，指针不是外键。这种模型按照路径遍历非常麻烦，更新也非常麻烦。

而使用了关系型数据库后，查询优化器会根据索引和表间关系，来生成“访问路径”-也就是执行计划。查询优化器是是一个被持续优化的怪兽。

文档数据库是某种意义上的层次模型-父文档保存了子文档。

文档型数据库的优点：性能更好，模型更像是程序自己的数据结构，**适合一对多模型**。

关系型数据库则强在 join、**多对一和多对多的表达上**。但，只要文档数据库可以通过标识符来引用其他文档，则文档数据库的表达能力并没有因而减弱。

如果原始数据有类似树型/层次/文档的复合结构，则比较适合使用文档数据库；否则应该对数据进行分解（规范化），得到关系型数据库的表。

通常，关系型表的数据结构相关的代码是更复杂的。但，如果需要引用嵌套的数据，则嵌套层次越深，文档型模型越不好用。

通常情况下，流式/批处理框架/消息队列里的 event，也适于使用文档数据库。事实上，除了订单系统里的订单/子订单以外，应该大量数据模型都可以放进文档型数据库里。

如果确实需要 join，则文档数据库的弊端就出现了。反范式化很难维护一致性，而且程序的流程会变复杂，流程变差了。

总而言之，关联性越高和数据库选型的关系是：文档型 -> 关系型 -> 图 数据库。

### 模式灵活性

应该说，文档型数据库有模式灵活性，它支持读时模式（与之相对地，关系型数据库支持的是写时模式）。文档型数据库往往不在写时执行强制模式校验，读时的兼容性必须由读时的应用程序来保证。

关系型数据库因为执行写时校验，所以出现模式变更时，往往需要成本很高的 migrate 操作。

如果外部模式很多，或者模式很易变-最典型的例子，配置型数据，则很适合使用文档型数据库；反之，关系型数据库则要被派上用场。**模式的损害在于，它不易于变动。**

### 数据局部性与性能

文档型数据库还有一个缺点，就是对它更新，需要**原地重写**，写的开销很大，可能引起存储问题。

### 融合的趋势

关系型数据库和文档数据库的融合是大势所趋。当代的 RDBMS 已经可以很好地处理 XML；而一部分的文档型数据库则可以在查询时支持 join（mongo 是在 client 端支持的，这种方案性能不够好，但支持也比不支持强）。 

## 数据查询语言

### 数据库里的查询语言

SQL 其实是种声明式查询语言，而 CODASYL 实际上是命令式。

命令式的查询语言，会把查询过程 HOW 写出来（所以我们经常做的客户端查询，都是命令式的查询），告诉计算机，要按照怎样的特定顺序，执行某些操作（第三步可以被扩展，扩展为 map reduce 的不断串联/并联执行）。

而声明式的查询语言，只会把 what 写出来（LINQ 最为明显），指定查询哪些模式，满足哪些条件，需要做怎样的数据处理/聚合。剩下的查询过程，由查询优化器来推导。

声明式的语言都有一个特性，就是无法/也不需要指定执行的流程的细节，这给了编译器/运行时重排执行流程，甚至并行化执行的机会。-**声明式其实是一种高级抽象，能够实现复杂查询流程的数据库，才能提供很漂亮的声明式查询语言，这体现了架构设计的一种取舍。**

MongoDB 里面的 AST 式的查询语言，本身只是重新发明了一遍 SQL 罢了。

### web 领域的查询语言

即使只在 Web 领域，CSS 代表的声明式语言，也比 JavaScript 代表的命令式查询要优雅很多。

### MapReduce 查询

MapReduce 起源于谷歌，MongoDB 和 couchDB 等文档型数据库也部分支持 MapReduce。

map 是函数式编程里的 collect，而 reduce 则是 fold 或者 inject。

MapReduce 不是声明式查询语言，也不是一个完全命令式的查询 API，而是介于两者之间：查询（及处理）的逻辑用代码片段来表示，这些代码片段会被框架来重用（代码片段的设计思路，也被用于 Stream 这项新兴技术中）。通常我们使用 map 来生成逻辑 KV，然后用 reduce 对相同的 Key 的 value 进行聚合处理。

map reduce 我们使用纯函数，因为没有副作用，所以纯函数的顺序和执行为之是非常自由的。

MapReduce 实际上是一种偏底层的编程模型，需要执行在计算集群上（否则性能并不好）。SQL 是极高层的计算模型，可以通过 MapReduce 来间接实现。当然，这两者之间并不必然有关系。

## 图计算模型

多对多模型是不同数据模型之间的重要区别特征。关系型数据库只适合处理简单的多对多关系，复杂的多对多关系需要使用图模型。

图包括顶点和边，常见的图有：

 - 社交网络
 - Web 图
 - 公路或铁路网
 
图的强大之处在于，它不仅可以存储同构数据，它提供了单个数据存储区中保存完全不同类型对象的一致性方式。

有多种不同但相关的方法可以构建和查询图中的数据，常见的图有属性图（property graph）和三元存储模型（triple-store），相关的查询语言有三种：Cypher、SPARQL 和 Datalog。

图计算模型比关系型数据库或者 CODASYL 更加自由，不需要指定 schema，而任何顶点都可以和其他顶点互联。

### 属性图

在属性图中，每个顶点包括：

 - 唯一的标识符
 - 出边的集合
 - 入边的集合
 - 属性的集合（键-值对）
 
每个边包括：
 - 唯一的标识符
 - 边开始的顶点（尾部的顶点）
 - 边结束的顶点（头部的顶点）
 - 描述两个顶点间关系类型的标签
 - 属性的集合（键-值对）
 
把这两种定义转化为 SQL，可以得到两张表：顶点表和边表。

```SQL
-- 顶点表
CREATE TABLE `vertices` (
  `id` bigint NOT NULL AUTO_INCREMENT COMMENT '物理主键',
  `properties` json DEFAULT NULL COMMENT '顶点属性，适应变化',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- 边表
CREATE TABLE `edges` (
  `id` bigint NOT NULL,
  `tail_vertex` bigint NOT NULL COMMENT '尾顶点',
  `head_vertex` bigint NOT NULL COMMENT '头顶点',
  `properties` json DEFAULT NULL COMMENT '属性',
  `label` varchar(45) NOT NULL COMMENT '关系类型的标签',
  PRIMARY KEY (`id`),
  KEY idx_tail_vertex (`tail_vertex`) USING BTREE,
  UNIQUE KEY idx_vertices (`head_vertex`,`tail_vertex`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
```

关于图模型还有一些值得注意的地方：

1. 任何顶点都可以连接到其他顶点。没有模式限制哪种事务可以或者不可以关联。
2. 给定某个顶点，可以高效地得到它的所有入边和出边，从而遍历图，即沿着这些顶点链条一直向前或者向后。
3. 通过对不同的类型的关系使用不同的标签，可以在单个图中存储多种不同类型的信息，同时仍然保持整洁的数据类型。-传统的关系模型难以表达不同国家的不同地区结构和颗粒度。

图是易于演化的，可以动态地往图里添加功能，图可以很容易适应并扩展。

图是一种前程远大，应用场景广泛的技术。

### Cypher 查询语言

Neo4j 是从黑客帝国里诞生的概念，Cypher 是另一个（和密码学里的 Cypher 恰巧同名），这两个名词都是从人名里诞生的。

我们可以先创建库和数据：

```SQL
CREATE 
(NAmerican: location, {name: 'North America', type: 'continent'}),
(USA: location, {name:'United States', type:'country'}),
(Idaho: location, {name:'Idaho', type:'state'}),
(Lucy: person, {name:'Lucy', type:''}),
(Idaho) -[:WITHIN] -> (USA) -[:WITHIN] -> (NAmerica),
(Lucy) - [:BORN_IN] -> (Idaho)
```

相应的查询语句是这样的：

```SQL
MATCH
(person) -[:BORN_IN] -> () - [:WITHIN*o..]-> (us: location, {name:'United States'}),
(person) -[:LIVES_IN] -> () - [:WITHIN*o..]-> (eu: location, {name:'Europe'}),
RETURN person.name
```

这里的式子分两层：第一层在右边，表明这是任意一个处于特定地点的地点，而第二层在左边，表明这是和第一层变量相关联的顶点。person 是一个待求值的变量。

遍历有两种基本思路：
- 从每个 person 开始，沿着出边过滤。
- 从 us 和 eu 这两个顶点开始，沿着入边过滤。

使用声明式的查询语句，可以让查询优化器自由地决定执行策略。

同样地，我们可以用关系型数据库来表达图数据库。但通常， SQL 查询要求我们能够制定 join 的次序和数量；对于图查询，join 操作的数量不是预先确定的。这种不能确定 join 顺序和次数的查询，容易诱发 SQL 的反模式。

WITHIN*o.. 的意思是，沿着 WITHIN 边，遍历 0 次或多次。

在 SQL 1999 中，查询这种可变的遍历路径，可以使用被称为**递归公用表表达式**（即 WITH_RECURSIVE 语法）来表示。

```SQL
-- head vertex 的意思是箭头，这个算法就是递归地从右向左找点（每找到一个点都把点添加进点集合里），而箭头本身带有 withhin 标签。
-- 这个声明式的算法，可以用类似图算法的方法，命令式地实现
WITH RECURSIVE
      -- in_usa 包含所有的美国境内的位置ID
        in_usa(vertex_id) AS (
        SELECT vertex_id FROM vertices WHERE properties ->> 'name' = 'United States'
        UNION
        SELECT edges.tail_vertex FROM edges
          JOIN in_usa ON edges.head_vertex = in_usa.vertex_id
          WHERE edges.label = 'within'
      ),
      -- in_europe 包含所有的欧洲境内的位置ID
        in_europe(vertex_id) AS (
        SELECT vertex_id FROM vertices WHERE properties ->> 'name' = 'Europe'
        UNION
        SELECT edges.tail_vertex FROM edges
          JOIN in_europe ON edges.head_vertex = in_europe.vertex_id
          WHERE edges.label = 'within' ),
      -- born_in_usa 包含了所有类型为Person，且出生在美国的顶点
        born_in_usa(vertex_id) AS (
          SELECT edges.tail_vertex FROM edges
            JOIN in_usa ON edges.head_vertex = in_usa.vertex_id
            WHERE edges.label = 'born_in' ),
      -- lives_in_europe 包含了所有类型为Person，且居住在欧洲的顶点。
        lives_in_europe(vertex_id) AS (
          SELECT edges.tail_vertex FROM edges
            JOIN in_europe ON edges.head_vertex = in_europe.vertex_id
            WHERE edges.label = 'lives_in')
      SELECT vertices.properties ->> 'name'
      FROM vertices
        JOIN born_in_usa ON vertices.vertex_id = born_in_usa.vertex_id
        JOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id;
```

从这个例子可以看出来，SQL 不如 Cypher，SQL 不具备找到一行记录后自递归的方法。

### 三元存储与 SPARQL

三元存储模型几乎等同于属性图模型，他们只是用不同的名词来描述了相同的思想。

在三元组中，所有信息都以非常简单的三部分形式存储（主语，谓语，客体）。

1. 三元组里主体相当于顶点，谓语和宾语相当于 proerpties 中的 key 和 value。
2. 三元组中的主体相当于顶点，谓语是途中的边，客体是头部顶点。

#### 语义网

Datomic 是一个三元组存储（其实是五元组，带有 2 元版本数据）。语义网（Semantic Network）不是三元组，语义网本身没有靠谱的实现，从未实际出现。

#### SPARQL

SPARQL（音“sparkle”）出现得比 Cypher 早，Cypher 的模式匹配是借用 SPARQL 的。

#### Datalog 出现得更早

Datalog 出现得更早，为 Cypher 和 SPARQL 奠定了基础。它是 Datomic 的查询语言。Datalog 是 Prolog 的一些子集。

## 小结

数据的模型发展的脉络，不过是：

树型 -> 文档 -> 关系模型 -> 图

关联越多，越适合使用后面的数据库。关系模型的平衡性最好，可以模拟其他数据模型。从这个顺序来讲，文档模型是关系模型在复杂性上的退化/或者简化。但唯有关系模型是强制使用模式的。

如果需求不断变化，模式可能不断变化，应该尽量选择无模式的数据模型。

每种模型都有自己的查询语言和框架。

这些模型在实现的时候，需要做一些权衡取舍。

# 数据存储和检索

从最基本的层面看，数据库只做两件事情：向它插入数据时，它就保存数据；之后查询时，它应该返回那些数据。

作为应用开发人员，我们大多数情况下不可能从头开始实现一个自己的存储引擎，往往需要从现存的存储引擎中选择一个适合自己的。其他针对事务型工作负载和针对分析型工作负载的存储引擎存在很大的差异。

我们将研究关系型数据库和 NoSQL 数据库，我们将研究两个存储引擎家族，即日志结构的存储引擎和面向页的存储引擎，比如 B-Tree（B-Tree 是页的组织）。

## 数据库核心：数据结构

数据存储里通常有三种数据结构：日志、页和索引。

### 日志

许多数据库内部都使用日志，日志是一种只支持追加式更新的数据文件。**一个数据库还要处理其他问题：并发控制、回收磁盘空间、错误处理和部分完成写记录等。**但日志始终是一个很有用的机制，被用在很多地方。

### 索引

在日志里面查找结果是不好的，所以引入第二种数据结构-索引。

索引的基本设计思想是，在原始数据上派生额外数据结构，在索引上保留关键数据或者元数据，作为路标，帮助定位想要的数据。

不同的索引支持不同的搜索方式。

索引必然导致写性能下降，因为索引很难使用追加写，但追加写是性能最高的写入方式。

#### 哈希索引

KV 结构随处可见，是构造更多更复杂索引的基础构造块。如，继承/封装 hashmap 是常见的存储方法。

一个特别简单粗暴的例子：Bitcask 的存储格式，使用 csv 来存储 kv 值，使用 hashmap 来存储 key 和文件系统里的 offset 来充当索引。

我们不能只依赖于一个数据文件，这会导致磁盘空间耗尽-**所以我们对大规模存储应该采取分段的形式。**

但使用多段数据，往往意味着数据需要压缩。压缩可以让段变小，因为段被合并后就不会再被修改，所以很适合放进新文件里。这样可以把旧文件段留出来，提供读写支持。

每个段都有自己的内存哈希表。-这里引出了一个范式，一段数据，到底在内存里是怎么被组织的，在硬盘里又是怎么被组织的，可以完全不一样。

这个方法是最简单的方法。但要在实践中行之有效，还要考虑如下问题：

1. 文件格式：CSV 不是最佳的文件格式，二进制才是。
2. 删除记录：如果要删除键和值，在日志里追加一个删除日志是简单的做法（墓碑）。墓碑会让记录在被合并时删除键值的实际内容。
3. 崩溃恢复：从头到尾读取日志是一个方法，快照内存里的 hashmap 是另一个方法-这和 RDB/AOF 的设计思想是很像的，快照可以加速崩溃恢复，快照本身就是 compact 过的值。
4. 部分写入的记录：文件要加上校验和。
5. 并发控制：只有一个写线程追加写入（类似 log4j 的设计），多个线程并发读。

为什么要使用追加写，而不是原地更新？这应该是几乎所有的存储使用日志配合数据页的解决方案需要回答的问题。

1. 追加写的顺序写性能好。
2. 顺序写的并发控制和崩溃恢复会简单得多-只有一个数据文件很难处理脏数据和正确的数据。
3. 有了文件合并，可以减少数据文件本身的碎片程度-所以数据文件本身还是 要紧凑，不能作为写的中间文件。

内存里的 hash 表有什么缺点？

1. （因为装填因子的存在）内存利用率不高。
2. 区间查询效率不高。

#### SSTables 和 LSM-Tree

SSTables 是排序字符串表，以它优化 Bitcask 的例子的话，会产生如下变化：

1. 每个的日志段里只能存在一个 kv 值，**不按照它们的写入顺序排序，而按照 key 的字典序排序，每个 key 只出现一次（这就像 TreeMap 了）。**
2. 段按照特定的时间段顺序排序，这也就意味着 compact 多个段的时候，可以按照时间的顺序读取同一个 key，只保留某个 key 的最新值，丢弃其他段里的其他值。
3. 在内存里保存的索引不需要指向所有的 key value 值，只要能够找到特定的段上的区间起止值，就可以找到特定的段上的最新的 kv，**这样我们可以得到一个稀疏的索引**。
4. 我们的值写入永远都是随机写入的，相关的存储引擎是这样工作的：
  1.  **写入先写入内存中的平衡数据结构**（通常是某种平衡树，如红黑树），这被叫作内存表（Mem table）。
  2.  当内存表的大小达到某个阈值以后，将其生成一个 SSTable 写入磁盘中，然后再生成下一个内存表继续供写入（这应该是一个原子切换）。
  3.  如果有读操作，先在内存表里查找，然后按照写顺序查找最新的磁盘段文件、次新的磁盘段文件，以此类推，直到找到目标（或为空）。
  4.  后台周期性地执行段合并与压缩过程，**以合并多个段文件并丢弃那些已被覆盖或者删除的值**。
5. 为了防止数据库崩溃，也要准备 WAL。WAL 使用**纯粹的追加写**，而不是排序写（这样的性能最好）。一段日志对应一段 WAL。

上述算法实质上是 LevelDB（Riak）和 RocksDB �所使用的，可以嵌入其它程序中提供 KV 存储。这两个引擎都收到 Google 的 BigTable 论文的影响（它引入了 SSTable 和 Mem table 两个术语）。

这种索引结构由 LSM-Tree 命名（这一章可能总体上被称作 LSM-Tree 算法）。Lucene 也使用类似的方案存储 term 和相关的 doc。

**总有很多细节，值得深入优化。**如果有个 Key 找不到，则 LSM-Tree 算法的表现可能很慢。这样可以引入布隆过滤器，近似计算集合的内容。

还有其他的策略可以影响甚至决定 SSTables 压缩和合并时的具体顺序和时机。最常见的方式是大小分级和分层压缩。分层压缩是 LevelDB 和 RocksDB 使用的策略，HBase 使用大小分级，Cassandra 同时支持这两种压缩。

在大小压缩中，较新的和较小的 SSTables 被连续合并到较旧和较大的 SSTables 里。

在分层压缩中，键的范围分裂成多个更小的 SSTables，旧数据被移动到单独的“层级”，这样压缩可以逐步并行并节省磁盘空间。

LSM 的基本思想（保存在后台合并的一系列 SSTable）足够简单有效。**即使数据集远远大于可用内存**，它仍然能够正常工作。由于磁盘是顺序写入的，LSM-Tree 可以支持非常高的写入吞吐量。

![LSM-Tree-Algorithm.jpg](LSM-Tree-Algorithm.jpg)

（要实现上面的结构，要特别考虑各种写操作的并发安全性，高性能还在其次）。

#### B-Tree

log-structure 日志索引结构正在逐渐受到更多的认可，但目前最广泛使用的索引结构是 B-Tree。

从 1970 诞生以来，B-Tree **经历了长久的时间的考验**，时至今日，它仍然是几乎所有关系型数据库中的标准索引实现。

B-Tree 也按 Key 对键值对排序（证明这个特性极端重要，对于存储而言，区间查找的重要性超乎想象地有用，这也是为什么 hash 类的存储结构使用场景不广泛的原因）。

B-Tree 首先把数据库分解成固定大小的块和页（因为操作系统通常使用块作为名称，所以数据库经常使用页），这样更接近底层硬件的设计。页是内部读写的最小单元。

每个页面都有自己的地址。

![B-Tree 的例子.png](B-Tree 的例子.png)

B-Tree 中一个页所包含的子页引用数量称为分支因子（branching factor）。这种设计使得 B-Tree 在 O(logn) 的深度上保持平衡。（如果 使用 B+ 树）一个分支因子为 500（通常应该为几百）的 4kb 页只要 4 层就能够存储 256tb 左右的数据。

如何使 B-Tree 更可靠？

对 B-Tree 的底层写操作基本上是使用新数据来代替磁盘上的旧页，（通常）这不会改变该页的磁盘存储位置。这与 LSM-tree 形成鲜明的对比，**后者只是追加写（实际上在 SSTable 内部还是排序更新文件，但总体上是追加写），新旧替换的操作发生在后台线程的 compact 里。**

为了做崩溃恢复，B-Tree 当然还是需要引入 WAL（而且 WAL 也会进化，从 binlog 进化到 redo log）。

如果要原地更新数据页，还要考虑并发控制问题，所以需要考虑锁存器（一种轻量级锁）；相比之下，日志结构化的方法先显得更简单了，因为它的实际原地更新是在后台发生的。

如何优化 B-Tree？

1. 引入 COW，SNAPSHOT Isolation。
2. 保存 key 的缩略信息，节省页空间，这样树具有更高的分支因子，从而减少层数（这就引入了 B+ 树）。
3. 尽量让相邻的页在磁盘上尽量连续。
4. 在叶子上添加额外的指针，这样寻找兄弟不需要找 parent（这也是 B+树的特性）。
5. 分形树引入了一些 log-structure 来减少磁盘寻道。

#### 对比 B-tree 和 LSM-tree

通常认为，LSM-tree 对写入更快，而 B-tree 对读写更快。


