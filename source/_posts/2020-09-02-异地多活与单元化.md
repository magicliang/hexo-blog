---
title: 异地多活与单元化
date: 2020-09-02 21:55:50
tags:
- 系统架构
---
# 问题定义

高可用架构要去除单点的潜在威胁。

涉及多数据中心、多地部署的设计的意义和价值很难在日常工作中体现出来，但体现了超前布局的思考。

所谓的单点包括：

- 单服务器
- 单应用
- 单数据库
- 单机房
- 单地部署

# 蚂蚁的单元化

## 引入服务治理

引入服务治理机制，使整个服务可以在物理上灵活扩展。

强依赖：服务注册中心/服务命名+查找机制。

在阿里的语境里，是由服务注册中心的中间件来进行流量调配的。

## 分库分表

**一个比较好的实践是：逻辑拆分先一步到位，物理拆分慢慢进行。**

> 以账户表为例，将用户 ID 的末两位作为分片维度，可以在逻辑上将数据分成 100 份，一次性拆到 100 个分表中。这 100
> 个分表可以先位于同一个物理库中，随着系统的发展，逐步拆成 2 个、5 个、10 个，乃至 100
> 个物理库。数据访问中间件会屏蔽表与库的映射关系，应用层不必感知。

换言之，蚂蚁的数据总是分成 100 个逻辑分片。至于这 100 个逻辑分片是 100 张表，还是会膨胀为 100 个库 * x 张表，都可以由数据库中间件进行屏蔽，且自由扩展。

强依赖：数据库中间件，把逻辑读写转换为物理表读写。

## 同城多机房

### 所有机房共用一个服务注册中心，所有的请求由它统一分发

> 要突破单机房的容量限制，最直观的解决办法就是再建新的机房，机房之间通过专线连成同一个内部网络。应用可以部署一部分节点到第二个机房，数据库也可以将主备库交叉部署到不同的机房。
> 这一阶段，只是解决了机房容量不足的问题，两个机房逻辑上仍是一个整体。

缺点：

> 1. 服务层逻辑上是无差别的应用节点，**每一次 RPC 调用都有一半的概率跨机房**；
> 2. 每个特定的数据库主库只能位于一个机房，所以宏观上也一定有一半的数据库访问是跨机房的。

### 每个机房独占一个服务注册中心，所有的请求独立分发

> 改进后的同城多机房架构，依靠不同服务注册中心，将应用层逻辑隔离开。只要一笔请求进入一个机房，应用层就一定会在一个机房内处理完。当然，由于数据库主库只在其中一边，所以这个架构仍然不解决一半数据访问跨机房的问题。

> 这个架构下，只要在入口处调节进入两个机房的请求比例，就可以精确控制两个机房的负载比例。基于这个能力，可以实现全站蓝绿发布。

## 两地三中心

城市 1：idc1（registry1 + leader） + idc2（registry2 + replica1）。所有的服务都访问 leader 数据库。
城市 2：idc3（registry3 + replica2） 另一个城市为备份的中心，访问replica，隔离对 leader 的依赖。

可以概括为同城热备，**异地冷备**。

> “两地三中心”是一种在金融系统中广泛应用的跨数据中心扩展与跨地区容灾部署模式，但也存在一些问题。异地灾备机房距离数据库主节点距离过远、访问耗时过长，异地备节点数据又不是强一致的，**所以无法直接提供在线服务**。

> 在扩展能力上，由于跨地区的备份中心不承载核心业务，不能解决核心业务跨地区扩展的问题；在成本上，灾备系统仅在容灾时使用，资源利用率低，成本较高；在容灾能力上，由于灾备系统冷备等待，容灾时可用性低，切换风险较大。

两地三中心看起来很美好，其实无法解决立刻切换的问题-**异地延迟无法消除**。

## 单元化

> 蚂蚁金服发展单元化架构的原始驱动力，可以概括为两句话：
> 
> 1. 异地多活容灾需求带来的数据访问耗时问题，量变引起质变；
> 2. 数据库连接数瓶颈制约了整体水平扩展能力，危急存亡之秋。

单元化的设想，涉及到业务层和核心层的分离（单一系统的业务层和核心层，中台层的业务中台和核心中台）：

![单元化的设想1.jpeg](单元化的设想1.jpeg)

> 单元化架构基于这样一种设想：如果应用层也能按照数据层相同的拆片维度，把整个请求链路收敛在一组服务器中，从应用层到数据层就可以组成一个封闭的单元。数据库只需要承载本单元的应用节点的请求，大大节省了连接数。“单元”可以作为一个相对独立整体来挪动，甚至可以把部分单元部署到异地去。

> 单元化有几个重要的设计原则：

> - **核心业务必须是可分片的**（有些业务是不可分片的：比如风控、营销的全局配置）
> - 必须保证核心业务的分片是均衡的，比如支付宝用用户 ID 作分片维度
> - 核心业务要尽量自包含，调用要尽量封闭
> - **整个系统都要面向逻辑分区设计，而不是物理部署**

逻辑上只分 10 个 RegionZone，每个 R Zone 承载 20 个逻辑分片。

![两地三中心的一种典型部署.jpeg](两地三中心的一种典型部署.jpeg)

注意，这种两地三中心的 ldc 之间存储的不是同构数据，而是异构数据，即一部分分片，这是一种自然而然的想法-现实中的 idc 通常全局都是同构的，且全局只有一个主同时存在。

![三地五中心的典型部署.jpeg](三地五中心的典型部署.jpeg)

进入三地五中心，正好每个中心一个 R Zone。

> 回到前面买早餐的例子，小王的 ID 是 12345666，分片号是 66，应该属于 Regional Zone 04；而张大妈 ID 是 yig54321233，分片号 33，应该属于 Regional Zone 02。

> 应用层会自动识别业务参数上的分片位，将请求发到正确的单元。业务设计上，我们会保证流水号的分片位跟付款用户的分片位保持一致，所以绝大部分微服务调用都会收敛在
> Regional Zone 04 内部。

> 但是转账操作一定会涉及到两个账户，很可能位于不同的单元。张大妈的账号就刚好位于另一个城市的 Regional Zone
> 02。当支付系统调用账务系统给张大妈的账号加钱的时候，就必须跨单元调用 Regional Zone 02
> 的账务服务。图中用红线表示耗时很长（几十毫秒级）的异地访问。

**涉及多个账户的操作，完全可能跨单元。**

城市级容灾的方案，强依赖于 OB 的选主、换主的机制：

![城市级容灾.jpeg](城市级容灾.jpeg)

这也意味着，不同城市的不同 idc能够承载多个 R zone的数据，进而激活多个 R zone 的中间件的流量切换规则。

> 一个城市整体故障的情况下，应用层流量通过规则的切换，由事先规划好的其他单元接管。
> 
> 数据层则是依靠自研的基于 Paxos 协议的分布式数据库
> OceanBase，自动把对应容灾单元的从节点选举为主节点，实现应用分片和数据分片继续收敛在同一单元的效果。我们之所以规划为“两地三中心”“三地五中心”这样的物理架构，实际上也是跟
> OceanBase 的副本分布策略息息相关的。数据层异地多活，又是另一个宏大的课题了，以后可以专题分享，这里只简略提过。
> 
> 这样，借助单元化异地多活架构，才能实现开头展示的“26 秒完成城市级容灾切换”能力。

强依赖的技术组件：

- DNS 层（最顶层基础设施层）
- 反向代理层（子网网关）
- 网关 /WEB 层（接入层网关）
- 服务层（可再分为业务层和核心层）
- 数据访问层。

单元化流量管控是一个自上而下的、复杂的、系统性工程：

![单元化流量管控.jpeg](单元化流量管控.jpeg)

> - DNS 层照理说感知不到任何业务层的信息，但我们做了一个优化叫“多域名技术”。比如 PC 端收银台的域名是 cashier.alipay.com，在系统已知一个用户数据属于哪个单元的情况下，就让其直接访问一个单独的域名，直接解析到对应的数据中心，避免了下层的跨机房转发。例如上图中的
> cashiergtj.alipay.com，gtj 就是内部一个数据中心的编号。移动端也可以靠下发规则到客户端来实现类似的效果。
> - 反向代理层是基于 Nginx 二次开发的，后端系统在通过参数识别用户所属的单元之后，在 Cookie 中写入特定的标识。下次请求，反向代理层就可以识别，直接转发到对应的单元。
> - 网关 /Web 层是应用上的第一道防线，是真正可以有业务逻辑的地方。在通用的 HTTP 拦截器中识别 Session 中的用户 ID 字段，如果不是本单元的请求，就 forward 到正确的单元。并在 Cookie 中写入标识，下次请求在反向代理层就可以正确转发。
> - 服务层 RPC 框架和注册中心内置了对单元化能力的支持，可以根据请求参数，透明地找到正确单元的服务提供方。
> - 数据访问层是最后的兜底保障，即使前面所有的防线都失败了，一笔请求进入了错误的单元，在访问数据库的时候也一定会去正确的库表，最多耗时变长，但绝对不会访问到错误的数据。

一般应用层或者业务中间件只能加上拦截器进行 forward/routing。

统一路由规则：

![统一路由规则.jpeg](统一路由规则.jpeg)

> 这么多的组件要协同工作，必须共享同一份规则配置信息。必须有一个全局的单元化规则管控中心来管理，并通过一个高效的配置中心下发到分布式环境中的所有节点。

> 规则的内容比较丰富，描述了城市、机房、逻辑单元的拓扑结构，更重要的是描述了分片 ID 与逻辑单元之间的映射关系。

![全局服务注册中心.jpeg](全局服务注册中心.jpeg)

> 服务注册中心内置了单元字段，所有的服务提供者节点都带有“逻辑单元”属性。不同机房的注册中心之间互相同步数据，最终所有服务消费者都知道每个逻辑单元的服务提供者有哪些。RPC
> 框架就可以根据需要选择调用目标。

注意看，上图的右边提供了一个细分的物理寻址的结构。


![注解驱动的rpc.jpeg](注解驱动的rpc.jpeg)

> RPC
> 框架本身是不理解业务逻辑的，要想知道应该调哪个单元的服务，信息只能从业务参数中来。如果是从头设计的框架，可能直接约定某个固定的参数代表分片
> ID，要求调用者必须传这个参数。但是单元化是在业务已经跑了好多年的情况下的架构改造，不可能让所有存量服务修改接口。要求调用者在调用远程服务之前把分片
> ID 放到 ThreadLocal 中？这样也很不优雅，违背了 RPC 框架的透明原则。
> 
> 于是我们的解决方案是框架定义一个接口，由服务提供方给出一个实现类，描述如何从业务参数中获取分片
> ID。服务提供方在接口上打注解，告诉框架实现类的路径。框架就可以在执行 RPC 调用的时候，根据注解的实现，从参数中截出分片
> ID。再结合全局路由规则中分片 ID 与逻辑单元之间的映射关系，就知道该选择哪个单元的服务提供方了。

这里通过接口指定了注解的参数的契约，用注解来解耦了配置对流程的入侵。用配置来减少对原流程契约的改造，使服务成为整体框架的一个插件。

## 容灾的基本步骤包括

自底向上激活、预热，切换流量，把有状态的服务先启动起来，然后启动无状态服务，然后切换其他中间件，最后切换流量调配规则。这样可以让慢的服务热到足够快，提供丝滑般的体验：

- 数据库切换
- 缓存容灾切换
- 多活规则切换
- 中间件切换
- 负载均衡切换
- 域名解析切换


参考文献：

1. [《蚂蚁金服异地多活的微服务体系》][1]

  [1]: https://www.sohu.com/a/304176787_472869