---
title: 系统的弹性
date: 2017-10-21 20:07:23
tags:
- 技术架构
- 分布式系统
---


背景介绍
----

&emsp;&emsp;1999年，Dan Kegel 在互联网上发表了一篇文章，首次将 [C10K][1] 问题带入软件工程师的视野。在那个互联网勃兴的年代，计算机的运算处理能力，ISP 能够提供的带宽和网速都还十分有限，用户的数量也很少（那时候一个网站几百个人是很正常的事）。Dan Kegel 却已经敏锐地注意到极端的场景下资源紧张的问题。按照他的观察，某些大型的网络站点需要面对高达10000个客户端的并行请求。以当时的通行系统架构，单机服务器并不足以处理这个这个问题（当时绝大部分系统也没有那么大的流量，所以大部分人也没意识到这个问题）。因此，系统设计者必须为 C10K 问题做好准备。在那篇文章之中， Dan Kegel 提出了使用非阻塞异步 IO 模型，和使用各种内核系统调用黑魔法来提高系统 IO 性能的方式，来提高单机的并行处理能力。不得不说，这篇文章在当时很有先驱意义，它使得大规模网络系统的流量问题浮上了水面，也让人们意识到了系统容量建模和扩容提升性能的重要性。在它的启发下，C10K 问题出现了很多变种，从并发 C10K clients，到并发 C10K connections，到 C10K concurrency，可谓百花齐放。针对这些问题，也出现了很多的解决方案：
&emsp;&emsp;cpu 密集型？上高频 CPU， 上多核，上多处理器，开多线程/进程。
&emsp;&emsp;io 密集型？换ssd。还不够？更改 IO 策略，Reactor/Proactor。调高系统参数（包括但不仅限于文件描述符等系统资源，tcp 协议栈队列大小等等）。windows 出现了 IOCP，Java 把 IO 更新换代，从 BIO 变成了 NIO/AIO。
&emsp;&emsp;内存密集型？换 OS，加内存条，使用池化内存，使用各种 kernal call（又是各种黑魔法）。
&emsp;&emsp;单机纵向扩容提升（scale up）处理能力有极限，那就来横向扩容提升（scale out）分布式处理。在系统上找一条竖线切开，化整为零，负载均衡，各种 Hash Mod，Round robin 轮番上阵。
&emsp;&emsp;时间过去十几年，系统设计师要解决的架构问题，恐怕已经是 [C1000K][2] 问题了。
&emsp;&emsp;时代的发展，并没有停步于此。
&emsp;&emsp;当今系统设计要面临的问题，出现了新的特点：
&emsp;&emsp;首先，总有些有限的资源，不像带宽 cpu 一样呼之即来，最典型的例子是火车票、天猫双十一时的秒杀iPad。谚语有云，一只舰队的航行速度，由其中最慢的舰船决定。高并发的千军万马，即使浩浩荡荡地通过了我们设计的各种数据链路，最终到达要争夺各种各样资源的使用权的地方--数据库。这种争夺的互斥性，为我们带来了各种各样的锁（不管是乐观的还是悲观的）。[锁是不可避免的][3]。而这种锁的存在，使得一个大型系统的 QPS 和 [QoS][4]，严重受到后端数据存储层的制约。相信很多人对常见的 RDBMS 都有各种各样的使用经验。不同场景下不同类型的数据库的 TPS 可以达到几千到上万，甚至几万。但可以明确的看到，这种性能效率，无法和 C10K-C1000K 的系统轻松对接。传统的关系型数据库从关系代数出发的各种范型理论，给其实现戴上了沉重的历史枷锁，大部分的 RDBMS 的性能提升空间的天花板很快就能看到。从阿姆达尔定律出发，这就是系统的不可扩展瓶颈。我们当然可以使用分布式存储或者 NoSQL 来缓解这一问题，但因为 [CAP][5] 定律和网络分区现象的存在，我们并不能根本改善虚幻的锁的困境。这种困境的存在，**使得一个很高 QPS 的系统的性能，会被后端  TPS 拉平**，因而 QPS 并不能无限推高。因为没有 TPS 1000K 的 RDBMS，真正的 C1000K 的系统恐怕是镜花水月，无法实现的。
&emsp;&emsp;其次，流量出现了分化。大部分的系统设计的额定性能，总有界限，但在某些场景下，**却会出现要求无限性能的需求**。因为带宽和上网设备变得廉价，制造海量网络流量在当今变得非常轻而易举。最典型的例子是，12306每年都饱受人肉 [DDoS][6] 攻击的困扰，因为火车票是一种紧俏资源，用户如果刷不到就回不了家，所以一个无效的请求会触发更多的无效请求。一个抢不到票的用户的行为模式会变得好像[肉鸡][7]一样，刷不到票就他会无限刷，一台电脑刷不到就换两台，不行再来手机刷，再不行去买抢票插件。网络时代变发达，用户可以发送请求的能力变得无比强大，火车座位却没有变多 ，12306的系统似乎设计得无论多高，都无法承载那么多流量（如果把12306全年的流量可视化出来，恐怕会看到一个非常尖锐的 Spike）。**一个很高 QPS 的系统，终究不是一个无限 QPS 的系统**。
&emsp;&emsp;最后，并没有必要刻意追求 C10K 到 C1000K的高流量设计。软件设计的艺术是掌控复杂性（Complexity），不要为了设计的性能指标使设计失控，无法维护。 一个系统的平均 QPS 和峰值 QPS 完全可以不是一个数量级。如何兼顾这两种运行模式，是一个很大的设计难题。因为为峰值的 QPS 准备的系统在空闲的时候会浪费很多资源，为了设计一个高 QPS 的系统已经带来了很多复杂性（见上面列举的方法），要设计一个弹性伸缩的系统，又要带来更多的复杂性。这些复杂性当然催生了很多新技术的诞生，比如各种弹性云，秒级启动的容器，各种虚拟化技术（[根据小道消息，亚马逊的云服务就是这样逼出来的][8]）。**但我们是不是真的有必要投入那么多的 effort，来追求这种尽善尽美**？逆取不得，且宜顺守。有的时候， worse is better。
&emsp;&emsp;溯游从之，道阻且长。溯洄从之，宛在水中央。我们也许可以停下追求 QPS 的脚步，尝试思考下如何用恒定的一般性能（比如，几千的 QPS？），来解决大并发问题。如果我们能够用一些简单的技巧来保护我们的系统，能够过滤掉无效的流量，进而满足极端场景下性能的可靠性需求。

如何保护系统
------


  [1]: http://www.kegel.com/c10k.html
  [2]: http://www.ideawu.net/blog/tag/c1000k
  [3]: http://www.ideawu.net/blog/tag/c1000k
  [4]: https://baike.baidu.com/item/qos/404053?fr=aladdin
  [5]: https://baike.baidu.com/item/CAP#3
  [6]: https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8B%92%E7%BB%9D%E6%9C%8D%E5%8A%A1%E6%94%BB%E5%87%BB/3802159?fromtitle=DDOS&fromid=444572
  [7]: https://baike.baidu.com/item/%E8%82%89%E9%B8%A1/33880
  [8]: https://www.quora.com/How-and-why-did-Amazon-get-into-the-cloud-computing-business-Rumor-has-it-that-they-wanted-to-lease-out-their-excess-capacity-outside-of-the-holiday-season-November%E2%80%93January-Is-that-true